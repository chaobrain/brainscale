{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 循环神经网络的在线学习\n",
    "\n",
    "在[关键概念](./concepts-zh.ipynb)一章中，我们介绍了`brainscale`在线学习的基础知识。在本节中，我们将讨论如何基于``brainscale``进行循环神经网络（Recurrent Neural Networks，RNNs）的在线学习。\n",
    "\n",
    "基于发放率的循环神经网络（Rate-based RNNs）相比于脉冲神经网络（Spiking Neural Networks，SNNs）更广泛地应用于目前的深度学习任务中。在这种网络中，神经元的输出是连续的浮点数，而非像SNNs中的离散脉冲。``brainscale``提供的``ParamDimVjpAlgorithm``算法可以非常高效地用于支持RNNs的在线学习。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59500c56c6226022"
  },
  {
   "cell_type": "code",
   "source": [
    "import brainstate\n",
    "import brainscale"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T10:28:41.336251Z",
     "start_time": "2025-07-21T10:28:40.023094Z"
    }
   },
   "id": "b02b853e9b4e093",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. RNNs基本概念\n",
    "\n",
    "循环神经网络是专门设计用于处理序列数据的神经网络架构。它们在自然语言处理、时间序列预测、语音识别等领域得到了广泛应用。\n",
    "\n",
    "**RNNs的数学模型**\n",
    "\n",
    "RNNs的一个关键特性是它们可以处理任意长度的输入序列，并且可以捕捉序列中的时间依赖关系。RNNs的典型结构如下：\n",
    "\n",
    "![RNNs](../_static/architecture-rnn-ltr.png)\n",
    "\n",
    "对于每个时间步$a^{<t>}$，激活$y^{<t>}$是由输入$x^{<t>}$和前一个时间步的激活$a^{<t-1>}$计算得到的。$a^{<t>}$和$y^{<t>}$的计算公式如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{<t>}=&g_{1}(W_{a a}a^{<t-1>}+W_{a x}x^{<t>}+b_{a}) \\\\\n",
    "y^{<t>}=&g_{2}(W_{y a}a^{<t>}+b_{y})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，${\\mathcal{W}}_{ax},\\;{\\mathcal{W}}_{a a},\\;{\\mathcal{W}}_{y a},\\;b_{a},\\;b_{y} $ 是时间上共享的系数，$g_1$和$g_2$ 是激活函数。在训练过程中，我们需要通过梯度下降算法来更新这些参数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d00a28c3312ea25b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**RNNs的应用**\n",
    "\n",
    "RNNs可以应用于多种不同的任务，包括：自然语言处理（NLP）、时间序列预测、语音识别等。下表总结了不同的应用领域：\n",
    "\n",
    "![RNNs Applications](../_static/rnn-applications.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5b19126d2eef473"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**RNNs的训练**\n",
    "\n",
    "RNNs的损失函数 $\\mathcal{L}$ 是根据每个时间步的损失定义的，如下所示：\n",
    "\n",
    "$$\n",
    "\\mathcal{L(\\hat{y},y)}=\\sum_{t=1}^{T_{y}}\\mathcal{L(\\hat{y}^{<t>},y^{<t>})}\n",
    "$$\n",
    "\n",
    "RNNs往往使用反向传播算法进行训练。在每个时间步，我们计算损失 $\\mathcal{L}$ 对权重矩阵 $W$ 的导数。在时间步 $T$，损失 $\\mathcal{L}$ 对权重矩阵 $W$ 的导数表示如下：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{L}}(T)}{\\partial W}=\\sum_{t=1}^{T}\\,\\frac{\\partial{\\mathcal{L}}(T)}{\\partial W}\\Big|_{(t)}\n",
    "$$\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faeb36e14fd8dc91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**RNNs的变体**\n",
    "\n",
    "为了解决传统RNN在处理序列数据时所面临的几个核心问题，包括梯度消失与梯度爆炸、信息流动控制、上下文理解和复杂数据处理等方面的局限性，研究者们提出了多种RNN的变体，其中最为流行的包括：\n",
    "\n",
    "1. 长短期记忆（LSTM）：长短期记忆网络（LSTM）由Hochreiter和Schmidhuber于1997年提出，旨在解决传统RNN在处理长期依赖关系时的梯度消失问题。\n",
    "\n",
    "LSTM单元由三个主要部分组成：\n",
    "\n",
    "- **遗忘门（Forget Gate）**：决定保留多少来自前一个隐藏状态的信息。\n",
    "  \n",
    "  $$\n",
    "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "  $$\n",
    "\n",
    "- **输入门（Input Gate）**：决定新信息的输入。\n",
    "  \n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  i_t =& \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\\\\n",
    "  \\tilde{C}_t =& \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "- **输出门（Output Gate）**：决定输出多少当前单元状态的信息。\n",
    "\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  o_t =& \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\\\\n",
    "  C_t =& f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t \\\\\n",
    "  h_t =& o_t \\cdot \\tanh(C_t)\n",
    "    \\end{aligned}\n",
    "  $$\n",
    "\n",
    "2. 门控循环单元（GRU）：是LSTM的简化版本，由Cho等人于2014年提出。GRU保留了LSTM的优势，但具有更少的参数。\n",
    "\n",
    "GRU只有两个门：\n",
    "\n",
    "- **重置门（Reset Gate）**：控制前一隐藏状态对当前状态的影响。\n",
    " \n",
    "  $$\n",
    "  r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "  $$\n",
    "\n",
    "- **更新门（Update Gate）**：决定当前单元状态的更新程度。\n",
    "\n",
    "  $$\n",
    "  z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "  $$\n",
    "\n",
    "- **单元状态更新**：\n",
    "\n",
    "  $$\n",
    "    \\begin{aligned}\n",
    "  \\tilde{h}_t =& \\tanh(W \\cdot [r_t \\cdot h_{t-1}, x_t] + b) \\\\\n",
    "  h_t =& (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h}_t\n",
    "    \\end{aligned}\n",
    "  $$\n",
    "\n",
    "\n",
    "GRU通过减少门的数量，使得模型更简单且训练更快，同时在许多任务上与LSTM表现相当。\n",
    "\n",
    "\n",
    "\n",
    "可以看到，这种数学形式下的RNN模型完美满足[关键概念](./concepts-zh.ipynb)中我们所说的“动力学”和\"动力学交互\"的分离 。因此 ``brainscale`` 的在线学习系统可以很好的支持RNN模型的在线学习。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97501c8aa34f6069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. ``brainscale``支持的RNN模型\n",
    "\n",
    "``brainscale``并非支持所有的RNN模型的在线学习。比如，对于最简单的Elman RNNs，\n",
    "\n",
    "$$\n",
    "h_t=f(W_hh_{t-1}+W_xx_t+b_h)\n",
    "$$\n",
    "\n",
    "``brainscale``并不支持它的在线学习。但是，对于LSTM和GRU等复杂的RNN模型，特别适合使用``brainscale``进行它们的在线学习。主要原因是LSTM和GRU等模型的状态更新是通过门控机制实现的，这些门控机制导致了丰富的状态变量内在动力学。正如[我们论文](https://doi.org/10.1101/2024.09.24.614728)中所展示的，``brainscale``的时序依赖关系的学习就是通过这些逐元素运算状态变量的动力学更新来实现的。\n",
    "\n",
    "目前，我们可以使用``ParamDimVjpAlgorithm``算法进行任意RNN模型的在线学习。让我们通过一个简单的例子来展示如何使用``brainscale``进行RNN的在线学习。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5f0f32048319d46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 基于复制任务的GRU模型在线学习\n",
    "\n",
    "\n",
    "### 3.1 复制任务数据集\n",
    "\n",
    "复制任务（Copying Task）是一种用于测试循环神经网络是否能够记住长时程依赖关系的经典任务。这种任务被广泛用于验证RNN模型在面对长序列信息时的表现，尤其是衡量模型是否能够有效地保留输入信息，避免梯度消失或梯度爆炸的影响。\n",
    "\n",
    "在典型的复制任务中，模型的目标是将给定的输入序列在一定延迟后准确复制到输出序列。例如，假设输入序列长度为$T$：\n",
    "\n",
    "- 输入序列的前面部分包含若干个特定符号（如字母或数字），然后跟随一定数量的占位符（如“0”），用以延迟输出的生成。\n",
    "- 序列的最后一部分则是一个信号，告知模型在接下来的时刻开始输出最初的若干个符号（即最初的输入子序列）。\n",
    "\n",
    "**举例说明**\n",
    "\n",
    "```bash\n",
    "输入序列示例：\n",
    "\"1 5 3 4 8 [等待符号] ... [等待符号] 标记符号 0 0 0 0 0\"\n",
    "期望输出：\n",
    "\"0 0 0 0 0 0 ... 0 1 5 3 4 8\"\n",
    "```\n",
    "\n",
    "该序列通常包含三部分:（1）需要被记忆的数字序列(如上例中的\"1 5 3 4 8\")，（2）中间的等待时间段(用特殊符号填充)，（3）触发输出的标记符号。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4671c95aa55d0dcb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面是一个简单的数据加载器示例："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb6b0fc14fa55cc4"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class CopyDataset:\n",
    "    def __init__(self, time_lag: int, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.seq_length = time_lag + 20\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            ids = np.zeros([self.batch_size, self.seq_length], dtype=int)\n",
    "            # 随机生成10个数字\n",
    "            ids[..., :10] = np.random.randint(1, 9, (self.batch_size, 10))\n",
    "            # 在输入序列最后10位中添加10个占位符\n",
    "            ids[..., -10:] = np.ones([self.batch_size, 10]) * 9\n",
    "            # 输入序列\n",
    "            x = np.zeros([self.batch_size, self.seq_length, 10])\n",
    "            for i in range(self.batch_size):\n",
    "                x[i, range(self.seq_length), ids[i]] = 1\n",
    "            yield x, ids[..., :10]\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T10:28:41.396338Z",
     "start_time": "2025-07-21T10:28:41.391899Z"
    }
   },
   "id": "8e424d50683027f8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "在上面的代码中，我们定义了一个``CopyDataset``类，我们可以使用``__iter__``方法来生成复制任务数据集。在每次迭代中，我们生成一个长度为`time_lag + 20`的序列，其中包含10个随机数字和`time_lag`个占位符。我们的目标是将这10个随机数字复制到输出序列中。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72d3fe0894384443"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 GRU模型定义\n",
    "\n",
    "我们使用GRU模型来解决复制任务。我们可以使用``brainscale``提供的``GRUCell``模型来定义GRU模型。下面是一个简单的GRU模型定义示例：\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e8d96c21e7232aa"
  },
  {
   "cell_type": "code",
   "source": [
    "class GRUNet(brainstate.nn.Module):\n",
    "    def __init__(self, n_in, n_rec, n_out, n_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        # 构建GRU多层网络\n",
    "        layers = []\n",
    "        for _ in range(n_layer):\n",
    "            layers.append(brainscale.nn.GRUCell(n_in, n_rec))\n",
    "            n_in = n_rec\n",
    "        self.layer = brainstate.nn.Sequential(*layers)\n",
    "        # 构建输出层\n",
    "        self.readout = brainscale.nn.Linear(n_rec, n_out)\n",
    "\n",
    "    def update(self, x):\n",
    "        return self.readout(self.layer(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T10:28:41.424834Z",
     "start_time": "2025-07-21T10:28:41.420523Z"
    }
   },
   "id": "8741d3139128d4a4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 在线学习\n",
    "\n",
    "接下来，我们首先写一个抽象``Trainer``类，使其能够根据指定的参数对复制任务进行模型训练。然后，我们将实现两个具体的训练器：``OnlineTrainer``和``BPTTTrainer``。``OnlineTrainer``使用``brainscale``的在线学习算法进行训练，而``BPTTTrainer``使用时间反向传播算法（BPTT）进行训练。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bc33bc4c6ce68bb"
  },
  {
   "cell_type": "code",
   "source": [
    "import braintools as bts\n",
    "from tqdm import tqdm\n",
    "import jax\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: brainstate.nn.Module,\n",
    "        opt: brainstate.optim.Optimizer,\n",
    "        n_epochs: int,\n",
    "        n_seq: int,\n",
    "        batch_size: int = 128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # target network\n",
    "        self.target = target\n",
    "\n",
    "        # optimizer\n",
    "        self.opt = opt\n",
    "        weights = self.target.states().subset(brainstate.ParamState)\n",
    "        opt.register_trainable_weights(weights)\n",
    "\n",
    "        # training parameters\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_seq = n_seq\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def batch_train(self, xs, ys):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def f_train(self):\n",
    "        dataloader = CopyDataset(self.n_seq, self.batch_size)\n",
    "        bar = tqdm(enumerate(dataloader), total=self.n_epochs)\n",
    "        losses = []\n",
    "        for i, (x_local, y_local) in bar:\n",
    "            if i == self.n_epochs:\n",
    "                break\n",
    "            # training\n",
    "            x_local = np.transpose(x_local, (1, 0, 2))\n",
    "            y_local = np.transpose(y_local, (1, 0))\n",
    "            r = self.batch_train(x_local, y_local)\n",
    "            bar.set_description(f'Training {i:5d}, loss = {float(r):.5f}', refresh=True)\n",
    "            losses.append(r)\n",
    "        return np.asarray(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T10:28:43.446447Z",
     "start_time": "2025-07-21T10:28:41.536980Z"
    }
   },
   "id": "c8e74073fe41d215",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "在上面的代码中，我们定义了一个抽象``Trainer``类，它包含一个``batch_train``方法，在一个批次的数据上训练模型。我们还定义了一个``f_train``方法，用于模型训练的直接入口。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b2b0d7819fddcfa"
  },
  {
   "cell_type": "code",
   "source": [
    "class OnlineTrainer(Trainer):\n",
    "    @brainstate.compile.jit(static_argnums=(0,))\n",
    "    def batch_train(self, inputs, target):\n",
    "        weights = self.target.states(brainstate.ParamState)\n",
    "\n",
    "        # 对于每一个batch的数据，重新初始化模型状态\n",
    "        brainstate.nn.init_all_states(self.target, inputs.shape[1])\n",
    "\n",
    "        # 初始化在线学习模型\n",
    "        # 此处，我们需要使用 mode 来指定使用数据集是具有 batch 维度的\n",
    "        model = brainscale.ParamDimVjpAlgorithm(self.target, mode=brainstate.mixin.Batching())\n",
    "\n",
    "        # 使用一个样例数据编译在线学习eligibility trace\n",
    "        model.compile_graph(inputs[0])\n",
    "\n",
    "        def _etrace_loss(inp, tar):\n",
    "            # call the model\n",
    "            out = model(inp)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = bts.metric.softmax_cross_entropy_with_integer_labels(out, tar).mean()\n",
    "            return loss, out\n",
    "\n",
    "        def _etrace_grad(prev_grads, x):\n",
    "            inp, tar = x\n",
    "            # 计算当前时刻的梯度\n",
    "            f_grad = brainstate.augment.grad(_etrace_loss, weights, has_aux=True, return_value=True)\n",
    "            cur_grads, local_loss, out = f_grad(inp, tar)\n",
    "            # 累计梯度\n",
    "            next_grads = jax.tree.map(lambda a, b: a + b, prev_grads, cur_grads)\n",
    "            # 返回累计后的梯度和损失函数值\n",
    "            return next_grads, (out, local_loss)\n",
    "\n",
    "        def _etrace_train(inputs_):\n",
    "            # 初始化梯度\n",
    "            grads = jax.tree.map(lambda a: jax.numpy.zeros_like(a), {k: v.value for k, v in weights.items()})\n",
    "            # 沿着时间轴计算和累积梯度\n",
    "            grads, (outs, losses) = brainstate.compile.scan(_etrace_grad, grads, (inputs_, target))\n",
    "            # 更新梯度\n",
    "            self.opt.update(grads)\n",
    "            return losses.mean()\n",
    "\n",
    "        # 在T时刻之前，模型更新其状态和eligibility trace\n",
    "        n_sim = self.n_seq + 10\n",
    "        brainstate.compile.for_loop(model, inputs[:n_sim])\n",
    "\n",
    "        # 在T时刻之后，模型开始在线学习\n",
    "        r = _etrace_train(inputs[n_sim:])\n",
    "        return r\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T10:28:43.463067Z",
     "start_time": "2025-07-21T10:28:43.456231Z"
    }
   },
   "id": "8c7858573a21c69a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "在上面的代码中，我们定义了一个``OnlineTrainer``类，它继承自``Trainer``类。其中，我们使用``brainscale.ParamDimVjpAlgorithm``算法来进行在线学习。在每个时间步，我们计算损失函数的梯度，并使用梯度下降算法来更新模型的参数。",
   "metadata": {
    "collapsed": false
   },
   "id": "919d87de295364ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 离线学习\n",
    "\n",
    "为了对比在线学习的性能，我们实现了一个``BPTTTrainer``类，它使用时间反向传播算法（BPTT）进行训练。下面是``BPTTTrainer``类的实现："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc971984d5f4f482"
  },
  {
   "cell_type": "code",
   "source": [
    "class BPTTTrainer(Trainer):\n",
    "    @brainstate.compile.jit(static_argnums=(0,))\n",
    "    def batch_train(self, inputs, targets):\n",
    "        # initialize the states\n",
    "        brainstate.nn.init_all_states(self.target, inputs.shape[1])\n",
    "\n",
    "        # 需要求解梯度的参数\n",
    "        weights = self.target.states(brainstate.ParamState)\n",
    "\n",
    "        def _run_step_train(inp, tar):\n",
    "            out = self.target(inp)\n",
    "            loss = bts.metric.softmax_cross_entropy_with_integer_labels(out, tar).mean()\n",
    "            return out, loss\n",
    "\n",
    "        def _bptt_grad_step():\n",
    "            # 在T时刻之前，模型更新其状态及其eligibility trace\n",
    "            n_sim = self.n_seq + 10\n",
    "            _ = brainstate.compile.for_loop(self.target, inputs[:n_sim])\n",
    "            # 在T时刻之后，模型开始在线学习\n",
    "            outs, losses = brainstate.compile.for_loop(_run_step_train, inputs[n_sim:], targets)\n",
    "            return losses.mean(), outs\n",
    "\n",
    "        # gradients\n",
    "        grads, loss, outs = brainstate.augment.grad(_bptt_grad_step, weights, has_aux=True, return_value=True)()\n",
    "\n",
    "        # optimization\n",
    "        self.opt.update(grads)\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T10:28:43.488631Z",
     "start_time": "2025-07-21T10:28:43.483119Z"
    }
   },
   "id": "98f04788e3c8f94a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 模型训练比较：在线学习 vs. 离线学习\n",
    "\n",
    "接下来，我们将在线学习和离线学习的性能进行比较。我们将模型训练100个epoch，并记录每个epoch的损失值。每个epoch包含128个样本，每个样本包含200个序列。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59eca21f99ddf0cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练在线学习模型："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71216aa7a19dc1cf"
  },
  {
   "cell_type": "code",
   "source": [
    "online = OnlineTrainer(\n",
    "    target=GRUNet(10, 200, 10, 1),\n",
    "    opt=brainstate.optim.Adam(0.001),\n",
    "    n_epochs=1000,\n",
    "    n_seq=200,\n",
    "    batch_size=128,\n",
    ")\n",
    "online_losses = online.f_train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T10:30:42.660812Z",
     "start_time": "2025-07-21T10:28:43.526751Z"
    }
   },
   "id": "d29c63c285d79963",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]D:\\codes\\projects\\brainscale\\brainscale\\_etrace_compiler_hid_param_op.py:363: UserWarning: \n",
      "Warning: The ETraceParam ('layer', 'layers', 0, 'Wr', 'weight_op') does not found the associated hidden states. \n",
      "We have changed is as a weight that is not trained with eligibility trace. However, if you \n",
      "found this is a compilation error, please report an issue to the developers at https://github.com/chaobrain/brainscale/issues. \n",
      "\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "D:\\codes\\projects\\brainscale\\brainscale\\_etrace_compiler_hid_param_op.py:363: UserWarning: \n",
      "Warning: The ETraceParam ('readout', 'weight_op') does not found the associated hidden states. \n",
      "We have changed is as a weight that is not trained with eligibility trace. However, if you \n",
      "found this is a compilation error, please report an issue to the developers at https://github.com/chaobrain/brainscale/issues. \n",
      "\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "Training    10, loss = 2.09268:   1%|          | 11/1000 [01:57<2:55:20, 10.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m online \u001B[38;5;241m=\u001B[39m OnlineTrainer(\n\u001B[0;32m      2\u001B[0m     target\u001B[38;5;241m=\u001B[39mGRUNet(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m200\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m      3\u001B[0m     opt\u001B[38;5;241m=\u001B[39mbrainstate\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(\u001B[38;5;241m0.001\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m,\n\u001B[0;32m      7\u001B[0m )\n\u001B[1;32m----> 8\u001B[0m online_losses \u001B[38;5;241m=\u001B[39m online\u001B[38;5;241m.\u001B[39mf_train()\n",
      "Cell \u001B[1;32mIn[4], line 44\u001B[0m, in \u001B[0;36mTrainer.f_train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     42\u001B[0m     y_local \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mtranspose(y_local, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m     43\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_train(x_local, y_local)\n\u001B[1;32m---> 44\u001B[0m     bar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m5d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mfloat\u001B[39m(r)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, refresh\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     45\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(r)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray(losses)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bdp\\Lib\\site-packages\\jax\\_src\\array.py:297\u001B[0m, in \u001B[0;36mArrayImpl.__float__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    295\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__float__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    296\u001B[0m   core\u001B[38;5;241m.\u001B[39mcheck_scalar_conversion(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 297\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__float__\u001B[39m()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bdp\\Lib\\site-packages\\jax\\_src\\profiler.py:333\u001B[0m, in \u001B[0;36mannotate_function.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m    331\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    332\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m TraceAnnotation(name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdecorator_kwargs):\n\u001B[1;32m--> 333\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    334\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m wrapper\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bdp\\Lib\\site-packages\\jax\\_src\\array.py:627\u001B[0m, in \u001B[0;36mArrayImpl._value\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_npy_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_fully_replicated:\n\u001B[1;32m--> 627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_npy_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_single_device_array_to_np_array()\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_npy_value\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mwriteable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_npy_value)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练BPTT模型："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "197e81415fc68f46"
  },
  {
   "cell_type": "code",
   "source": [
    "bptt = BPTTTrainer(\n",
    "    target=GRUNet(10, 200, 10, 1),\n",
    "    opt=brainstate.optim.Adam(0.001),\n",
    "    n_epochs=1000,\n",
    "    n_seq=200,\n",
    "    batch_size=128,\n",
    ")\n",
    "bptt_losses = bptt.f_train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T10:30:42.718476600Z",
     "start_time": "2024-11-24T08:00:35.762066Z"
    }
   },
   "id": "9de3e47f4f16715f",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "比较在线学习和离线学习的训练损失。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb5acad00504c3b6"
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(online_losses, label='Online Learning')\n",
    "plt.plot(bptt_losses, label='BPTT')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T10:30:42.838189400Z",
     "start_time": "2024-11-24T08:20:12.829536Z"
    }
   },
   "id": "8d02083c8e019b23",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 总结\n",
    "\n",
    "\n",
    "这份教程全面介绍了如何使用`brainscale`框架实现循环神经网络的在线学习。值得注意的是，`brainscale`特别适合像LSTM和GRU这样具有门控机制的复杂RNN模型。对于像Vanilla RNN这样只有权重交互导致recurrence的RNN模型，`brainscale`在线学习系统很难支持精确的梯度计算。\n",
    "\n",
    "我们希望这份教程能帮助研究人员和工程师更好地将``brainscale``的在线学习系统用于解决RNNs实际工程训练中的问题。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7a9ce15304f651"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
