{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Key Concepts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28712d66ca25b58f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Welcome to `brainscale`!\n",
    "\n",
    "BrainScale is a Python library designed for implementing online learning in neural network models with dynamics. Online learning represents a learning paradigm that enables continuous parameter updates as models receive new data streams. This approach proves particularly valuable in real-world applications, including robotic control systems, agent decision-making processes, and large-scale data stream processing.\n",
    "\n",
    "\n",
    "In this section, I will introduce some of the key concepts that are fundamental to understanding and using online learning methods defined in ``brainscale`` . These concepts include:\n",
    "\n",
    "- Concepts related to build high-Level Neural Networks.\n",
    "- Concepts related to customize neural network module: ``ETraceVar`` for hidden states, ``ETraceParam`` for weight parameters, and ``ETraceOp`` for input-to-hidden transition.\n",
    "- Concepts for online learning algorithms ``ETraceAlgorithm``.\n",
    "\n",
    "``brainscale`` is seamlessly integrated in the [brain dynamics programming ecosystem](https://ecosystem-for-brain-dynamics.readthedocs.io/) centred on ``brainstate``. We strongly recommend that you first familiarise yourself with [basic usage of ``brainstate``](https://brainstate.readthedocs.io/), as this will help you better understand how ``brainscale`` works.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9efa89155606dc33"
  },
  {
   "cell_type": "code",
   "source": [
    "import brainstate as bst\n",
    "import brainunit as u\n",
    "\n",
    "import brainscale"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:09.500491Z",
     "start_time": "2024-11-24T07:25:07.803942Z"
    }
   },
   "id": "848fc7b2e198e56f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Dynamical Models Supported in ``brainscale``\n",
    "\n",
    "``BrainScale`` does not support online learning for arbitrary dynamical models. The dynamical models currently supported by ``BrainScale`` exhibit a specific architectural constraint, as illustrated in the figure below, wherein the \"dynamics\" and \"interactions between dynamics\" are strictly segregated. Models adhering to this architecture can be decomposed into two primary components:\n",
    "\n",
    "- **Dynamics**: This component characterizes the intrinsic dynamics of neurons, encompassing models such as the Leaky Integrate-and-Fire (LIF) neuron model, the FitzHugh-Nagumo model, and Long Short-Term Memory (LSTM) networks. The update of dynamics (hidden states) is implemented through strictly element-wise operations, although the model may incorporate multiple hidden states.\n",
    "\n",
    "- **Interaction between Dynamics**: This component defines the interactions between neurons, implemented through weight matrices or connectivity matrices. The interactions between model dynamics can be realized through standard matrix multiplication, convolutional operations, or sparse operations.\n",
    "\n",
    "![](../_static/model-dynamics-supported.png)\n",
    "\n",
    "\n"
   ],
   "id": "260b588265bf6265"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To elucidate the class of dynamical models supported by BrainScale, let us examine a fundamental Leaky Integrate-and-Fire (LIF) neural network model. The dynamics of this network are governed by the following differential equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tau \\frac{dv_i}{dt} &= -v_i + I_{\\text{ext}} + s_i \\\\\n",
    "\\tau_s \\frac{ds_i}{dt} &= -s_i + \\sum_{j} w_{ij} \\delta(t - t_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, $v_i$ represents the membrane potential of the neuron. When this potential exceeds a threshold value $v_{th}$, the neuron generates an action potential and its membrane potential is reset to $v_{\\text{reset}}$, as described by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_i & = \\mathcal{H}(v_i-v_{th}) \\\\\n",
    "v_i & \\leftarrow v_{\\text{reset}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Additionally, $s_i$ denotes the postsynaptic current, $I_{\\text{ext}}$ represents the external input current, $w_{ij}$ is the synaptic weight from neuron $i$ to neuron $j$, and $\\delta(t - t_j)$ is the Dirac delta function indicating the reception of a synaptic event at time $t_j$. The time constants $\\tau$ and $\\tau_s$ characterize the temporal evolution of the membrane potential and postsynaptic current, respectively.\n",
    "\n",
    "Through numerical integration, we discretize the above differential equations and express them in vector form, yielding the following update rules:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{v}_i^{t+1} &= \\mathbf{v}_i^{t} + \\frac{\\Delta t}{\\tau} (-\\mathbf{v}_i^{t} + \\mathbf{I}_{\\text{ext}} + \\mathbf{s}^t) \\\\\n",
    "\\mathbf{s}_i^{t+1} &= \\mathbf{s}_i^{t} + \\frac{\\Delta t}{\\tau_s} (-\\mathbf{s}_i^{t} + \\underbrace{  W \\mathbf{z}^t  } _ {\\text{neuronal interaction}} )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notably, the dynamics of the LIF neurons are updated through element-wise operations, while the interaction component is implemented via matrix multiplication. All dynamical models supported by BrainScale can be decomposed into similar `dynamics` and `interaction` components. It is particularly worth noting that this architecture encompasses the majority of recurrent neural network models, thus enabling BrainScale to support online learning for a wide range of recurrent neural network architectures."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36152a55249cdebd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. `brainscale.nn`: Constructing Neural Networks with Online Learning Support\n",
    "\n",
    "The construction of neural network models supporting online learning in BrainScale follows identical conventions as those employed in `brainstate`. For comprehensive tutorials, please refer to the documentation on [Building Artificial Neural Networks](https://brainstate.readthedocs.io/en/latest/tutorials/artificial_neural_networks-en.html) and [Building Spiking Neural Networks](https://brainstate.readthedocs.io/en/latest/tutorials/spiking_neural_networks-en.html).\n",
    "\n",
    "The sole distinction lies in the requirement to utilize components from the [`brainscale.nn` module](../apis/nn.rst) for model construction. These components represent extensions of `brainstate.nn` module, specifically engineered to provide modular units with online learning capabilities.\n",
    "\n",
    "Below, we present a basic implementation demonstrating the construction of a Leaky Integrate-and-Fire (LIF) neural network using the `brainscale.nn` module."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c146839ccad9ff46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:09.520170Z",
     "start_time": "2024-11-24T07:25:09.513500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LIF_Delta_Net(bst.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in,\n",
    "        n_rec,\n",
    "        tau_mem=5. * u.ms,\n",
    "        V_th=1. * u.mV,\n",
    "        spk_fun=bst.surrogate.ReluGrad(),\n",
    "        spk_reset: str = 'soft',\n",
    "        rec_scale: float = 1.,\n",
    "        ff_scale: float = 1.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Using the LIF model in brainscale.nn\n",
    "        self.neu = brainscale.nn.LIF(n_rec, tau=tau_mem, spk_fun=spk_fun, spk_reset=spk_reset, V_th=V_th)\n",
    "\n",
    "        # Constructing input and recurrent connection weights\n",
    "        rec_init = bst.init.KaimingNormal(rec_scale, unit=u.mV)\n",
    "        ff_init = bst.init.KaimingNormal(ff_scale, unit=u.mV)\n",
    "        w_init = u.math.concatenate([ff_init([n_in, n_rec]), rec_init([n_rec, n_rec])], axis=0)\n",
    "\n",
    "        # Using delta synaptic projections to construct input and recurrent connections\n",
    "        self.syn = bst.nn.DeltaProj(\n",
    "            # Using the Linear model in brainscale.nn\n",
    "            comm=brainscale.nn.Linear(n_in + n_rec, n_rec, w_init=w_init, b_init=bst.init.ZeroInit(unit=u.mV)),\n",
    "            post=self.neu\n",
    "        )\n",
    "\n",
    "    def update(self, spk):\n",
    "        inp = u.math.concatenate([spk, self.neu.get_spike()], axis=-1)\n",
    "        self.syn(inp)\n",
    "        self.neu()\n",
    "        return self.neu.get_spike()"
   ],
   "id": "298e843a447e8e6e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exemplar implementation, we define a `LIF_Delta_Net` class that inherits from `bst.nn.Module`. The architecture incorporates two primary components: a Leaky Integrate-and-Fire (LIF) neuron model implemented as `self.neu`, and a `DeltaProj` module designated as `self.syn` which manages both input and recurrent connectivity.\n",
    "\n",
    "Subsequently, we shall proceed to construct a three-layer Gated Recurrent Unit (GRU) neural network model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ffc1c4945fa7cf2"
  },
  {
   "cell_type": "code",
   "source": [
    "class GRU_Net(bst.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        n_rec: int,\n",
    "        n_out: int,\n",
    "        n_layer: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Building the GRU Layer\n",
    "        self.layers = []\n",
    "        for i in range(n_layer - 1):\n",
    "            # Using the GRUCell model within brainscale.nn\n",
    "            self.layers.append(brainscale.nn.GRUCell(n_in, n_rec))\n",
    "            n_in = n_rec\n",
    "        self.layers.append(brainscale.nn.GRUCell(n_in, n_out))\n",
    "\n",
    "    def update(self, x):\n",
    "        # Updating the GRU Layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:09.530639Z",
     "start_time": "2024-11-24T07:25:09.526519Z"
    }
   },
   "id": "f482d8336349bd33",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "As demonstrated, the process of constructing neural network models using the [`brainscale.nn` module](../apis/nn.rst) maintains complete procedural equivalence with the construction methodology employed in the [`brainstate.nn` module](https://brainstate.readthedocs.io/en/latest/apis/nn.html). This architectural parallelism enables direct utilization of existing `brainstate` tutorials for developing neural network models with online learning capabilities."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d4dbf7a96571156"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. `ETraceState`, `ETraceParam`, and `ETraceOp`: Customizing Network Modules\n",
    "\n",
    "While the `brainscale.nn` module provides fundamental network components, it does not encompass all possible network dynamics. Consequently, BrainScale implements a mechanism for customizing module development through three primary classes: `ETraceState`, `ETraceParam`, and `ETraceOp`.\n",
    "\n",
    "**Core Components**\n",
    "\n",
    "- **`brainscale.ETraceState`**: Represents the hidden states $\\mathbf{h}$ within modules, defining dynamical states such as membrane potentials in LIF neurons or postsynaptic conductances in exponential synaptic models.\n",
    "\n",
    "- **`brainscale.ETraceParam`**: Corresponds to model parameters $\\theta$ within modules, encompassing elements such as weight matrices for linear matrix multiplication and adaptive time constants in LIF neurons. All parameters requiring gradient updates during training must be defined within `ETraceParam`.\n",
    "\n",
    "- **`brainscale.ETraceOp`**: Describes the operations that transform input data into postsynaptic currents based on model parameters. Supports various computational operations including linear matrix multiplication, sparse matrix operations, and convolution operations.\n",
    "\n",
    "**Foundational Framework**\n",
    "\n",
    "These three components—`ETraceState`, `ETraceParam`, and `ETraceOp`—constitute the fundamental conceptual framework underlying neural network models with online learning capabilities in BrainScale.\n",
    "\n",
    "In the following sections, we will present a series of illustrative examples demonstrating the practical implementation of custom network modules using `ETraceState`, `ETraceParam`, and `ETraceOp`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8fb5d8ec76b1ee3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 `ETraceState`: Model States\n",
    "\n",
    "Let us first examine a fundamental Leaky Integrate-and-Fire (LIF) neuron model, whose dynamics are characterized by the following differential equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tau \\frac{dv_i}{dt} &= -v_i + I_{\\text{ext}} + v_\\text{rest} \\\\\n",
    "z_i & = \\mathcal{H}(v_i-v_{th}) \\\\\n",
    "v_i & \\leftarrow v_{\\text{reset}} \\quad \\text{if} \\quad z_i > 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, $v_i$ represents the membrane potential of the neuron. When this potential exceeds a threshold value $v_{th}$, the neuron generates an action potential, and its membrane potential is reset to $v_{\\text{reset}}$. The Heaviside function $\\mathcal{H}$ characterizes the action potential generation, while $I_{\\text{ext}}$ denotes the external input current. The temporal evolution of the membrane potential is governed by the time constant $\\tau$, and $v_\\text{rest}$ represents the resting membrane potential."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c89a04bcad72713d"
  },
  {
   "cell_type": "code",
   "source": [
    "import jax\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class LIF(bst.nn.Neuron):\n",
    "    \"\"\"\n",
    "    Leaky integrate-and-fire neuron model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: bst.typing.Size,\n",
    "        R: bst.typing.ArrayLike = 1. * u.ohm,\n",
    "        tau: bst.typing.ArrayLike = 5. * u.ms,\n",
    "        V_th: bst.typing.ArrayLike = 1. * u.mV,\n",
    "        V_reset: bst.typing.ArrayLike = 0. * u.mV,\n",
    "        V_rest: bst.typing.ArrayLike = 0. * u.mV,\n",
    "        V_initializer: Callable = bst.init.Constant(0. * u.mV),\n",
    "        spk_fun: Callable = bst.surrogate.ReluGrad(),\n",
    "        spk_reset: str = 'soft',\n",
    "        name: str = None,\n",
    "    ):\n",
    "        super().__init__(size, name=name, spk_fun=spk_fun, spk_reset=spk_reset)\n",
    "\n",
    "        # parameters\n",
    "        self.R = bst.init.param(R, self.varshape)\n",
    "        self.tau = bst.init.param(tau, self.varshape)\n",
    "        self.V_th = bst.init.param(V_th, self.varshape)\n",
    "        self.V_rest = bst.init.param(V_rest, self.varshape)\n",
    "        self.V_reset = bst.init.param(V_reset, self.varshape)\n",
    "        self.V_initializer = V_initializer\n",
    "\n",
    "    def init_state(self, batch_size: int = None, **kwargs):\n",
    "        # Here is the most critical step, we define an ETraceState class \n",
    "        # that describes the kinetic state of membrane potentials\n",
    "        self.V = brainscale.ETraceState(bst.init.param(self.V_initializer, self.varshape, batch_size))\n",
    "\n",
    "    def reset_state(self, batch_size: int = None, **kwargs):\n",
    "        self.V.value = bst.init.param(self.V_initializer, self.varshape, batch_size)\n",
    "\n",
    "    def get_spike(self, V=None):\n",
    "        V = self.V.value if V is None else V\n",
    "        v_scaled = (V - self.V_th) / (self.V_th - self.V_reset)\n",
    "        return self.spk_fun(v_scaled)\n",
    "\n",
    "    def update(self, x=0. * u.mA):\n",
    "        last_v = self.V.value\n",
    "        lst_spk = self.get_spike(last_v)\n",
    "        V_th = self.V_th if self.spk_reset == 'soft' else jax.lax.stop_gradient(last_v)\n",
    "        V = last_v - (V_th - self.V_reset) * lst_spk\n",
    "        # membrane potential\n",
    "        dv = lambda v: (-v + self.V_rest + self.R * self.sum_current_inputs(x, v)) / self.tau\n",
    "        V = bst.nn.exp_euler_step(dv, V)\n",
    "        V = self.sum_delta_inputs(V)\n",
    "        self.V.value = V\n",
    "        return self.get_spike(V)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:09.847563Z",
     "start_time": "2024-11-24T07:25:09.837970Z"
    }
   },
   "id": "b3b5b27ce681d400",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the code above, we implement the `LIF` model through inheritance from `brainstate.nn.Neuron`. The class incorporates an `ETraceState` class variable `self.V` that characterizes the dynamical state of the membrane potential. The `init_state` method establishes the initial conditions for the membrane potential dynamics, while the `update` method implements the temporal evolution of these dynamics.\n",
    "\n",
    "This implementation maintains substantial structural similarity with the `LIF` class definition in `brainstate`, with one crucial distinction: whereas `brainstate` employs `brainstate.HiddenState` to represent the membrane potential dynamics, `brainscale` utilizes `brainstate.ETraceState` to explicitly designate this dynamical state for online learning applications."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "349531d8061ba2c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Therefore, we say that `brainscale.ETraceState` can be conceptualized as the counterpart to `brainstate.HiddenState`, specifically designed for defining model states that require eligibility trace updates. Should model states be defined using `brainstate.HiddenState` rather than `brainscale.ETraceState`, the online learning compiler in `brainscale` will fail to recognize these states. This oversight results in the compiled online learning rules being ineffective for the affected states, potentially leading to erroneous or omitted gradient updates in the model.\n",
    "\n",
    "But we should still be aware that there are obvious differences between `ETraceState` and `HiddenState`:\n",
    "- `brainscale.ETraceState`: Explicitly marks states for eligibility trace computation\n",
    "- `brainstate.HiddenState`: Standard state representation within ``branstate`` without online learning capabilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acdc51aa04614f01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 `ETraceParam`: Model Parameters\n",
    "\n",
    "Let us examine a fundamental matrix multiplication operator, defined by the following mathematical expression:\n",
    "\n",
    "$$\n",
    "y = W x + b\n",
    "$$\n",
    "\n",
    "where $W$ represents the weight matrix, $x$ denotes the input vector, and $b$ is the bias vector. These model parameters can be defined using the `ETraceParam` class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8a5e0e6bc147653"
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_weight(\n",
    "    n_in, n_out, init: Callable = bst.init.KaimingNormal()\n",
    ") -> brainscale.ETraceParam:\n",
    "    weight = init([n_in, n_out])\n",
    "    bias = bst.init.ZeroInit()([n_out])\n",
    "    \n",
    "    # Here is the most crucial step, we define an ETraceParam class to describe the weight matrix and bias vector\n",
    "    return brainscale.ETraceParam({'weight': weight, 'bias': bias})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:09.860677Z",
     "start_time": "2024-11-24T07:25:09.856918Z"
    }
   },
   "id": "67a818a8b0a746f5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the code above, we define a `generate_weight` function that produces weight matrices and bias vectors. This function returns an `ETraceParam` object that encapsulates these parameters.\n",
    "\n",
    "`brainscale.ETraceParam` serves as the counterpart to `brainstate.ParamState`, specifically designed for model parameters requiring eligibility trace updates. When model parameters $\\theta$ are defined using `brainscale.ETraceParam`, the online learning compiler in `brainscale` implements temporally-dependent gradient updates according to the following formula:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}=\\sum_{t} \\frac{\\partial \\mathcal{L}^{t}}{\\partial \\mathbf{h}^{t}} \\sum_{k=1}^t \\frac{\\partial \\mathbf{h}^t}{\\partial \\boldsymbol{\\theta}^k},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\theta}^k$ represents the weight $\\boldsymbol{\\theta}$ utilized at time step $k$.\n",
    "\n",
    "Conversely, when model parameters $\\theta$ are defined using `brainstate.ParamState`, the online learning compiler computes only the instantaneous gradient of the loss function with respect to the weights:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}=\\sum_{t} \\frac{\\partial \\mathcal{L}^{t}}{\\partial \\mathbf{h}^{t}} \\frac{\\partial \\mathbf{h}^t}{\\partial \\boldsymbol{\\theta}^t}.\n",
    "$$\n",
    "\n",
    "This implementation distinction signifies that in `brainscale`'s online learning framework, parameters defined as `brainstate.ParamState` are treated as those not requiring eligibility trace updates, thereby forfeiting the ability to compute gradients with temporal dependencies. This architectural design enhances the flexibility of parameter update patterns, thereby increasing the customizability of gradient computation mechanisms."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea5917d98b885877"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 `ETraceOp`: Model Input-Output Functions\n",
    "\n",
    "`ETraceOp` represents another fundamental concept in dynamics interaction. While `ETraceParam` characterizes the parameters utilized in dynamical interactions, `ETraceOp` defines the operational transformations of these interactions. It must be implemented as a function adhering to the following specification:\n",
    "\n",
    "```python\n",
    "def op(\n",
    "    x: jax.Array, \n",
    "    param: brainscale.ETraceParam\n",
    ") -> jax.Array:\n",
    "    pass\n",
    "```\n",
    "\n",
    "`ETraceOp` describes the transformation of model inputs to outputs based on specified parameters. This framework supports various computational operations, including:\n",
    "- Linear matrix multiplication\n",
    "- Sparse matrix operations\n",
    "- Convolution operations\n",
    "- and more\n",
    "\n",
    "For the matrix multiplication operator discussed above, we can define an appropriate `ETraceOp` as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7478130b83dccc0"
  },
  {
   "cell_type": "code",
   "source": [
    "@brainscale.ETraceOp\n",
    "def matmul(x, w):\n",
    "    weight = w['weight']\n",
    "    bias = w['bias']\n",
    "    return x @ weight + bias"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:09.870180Z",
     "start_time": "2024-11-24T07:25:09.866525Z"
    }
   },
   "id": "4620fe96e8b896c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 `ETraceParamOp` = `ETraceParam` + `ETraceOp`: Integration of Model Parameters and Operations\n",
    "\n",
    "It becomes evident that `ETraceParam` and `ETraceOp` are inherently coupled concepts. `ETraceParam` cannot be parameterized independently of functional transformations, while `ETraceOp` cannot perform input-output transformations without parameterization. This intrinsic relationship motivates the introduction of a unified concept: `ETraceParamOp`.\n",
    "\n",
    "`ETraceParamOp` encapsulates both model parameters and their associated operations, providing a comprehensive framework for describing parameter-operation combinations. During instantiation, it requires both a parameter object and an operation function as inputs.\n",
    "\n",
    "Using the `ETraceParamOp` framework, we can reformulate the matrix multiplication operator discussed above as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aef04a53abc80dd6"
  },
  {
   "cell_type": "code",
   "source": [
    "class Linear(bst.nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_size: bst.typing.Size,\n",
    "        out_size: bst.typing.Size,\n",
    "        w_init: Callable = bst.init.KaimingNormal(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # input and output shape\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        # weights\n",
    "        weight = bst.init.param(w_init, [self.in_size[-1], self.out_size[-1]], allow_none=False)\n",
    "        \n",
    "        # operation\n",
    "        op = lambda x, w: u.math.matmul(x, w)\n",
    "        \n",
    "        # Here is the most crucial step, we define an ETraceParamOp class to describe the weight matrix and operations\n",
    "        self.weight_op = brainscale.ETraceParamOp(weight, op)\n",
    "\n",
    "    def update(self, x):\n",
    "        # Operation of ETraceParamOp\n",
    "        return self.weight_op.execute(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:09.880081Z",
     "start_time": "2024-11-24T07:25:09.876199Z"
    }
   },
   "id": "43454dfe8ceaa4c9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "As demonstrated in the code above, `ETraceParamOp` represents an integrated construct that unifies model parameters and their associated operations. This unified framework requires both a parameter object and an operation function for instantiation. The `execute` method of `ETraceParamOp` implements the operational transformation, converting input data into output data according to the specified parameters and operations.\n",
    "\n",
    "Through this implementation, we have successfully defined a fundamental `Linear` layer module, encompassing a weight matrix and its corresponding matrix multiplication operation. This module serves as a building block for constructing neural network models with online learning capabilities."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1734fdf3d1fbdf5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. `ETraceAlgorithm`: Online Learning Algorithms\n",
    "\n",
    "`ETraceAlgorithm` represents another fundamental concept in the BrainScale framework, defining both the eligibility trace update process during model state evolution and the gradient update rules for model parameters. Implemented as an abstract class, `ETraceAlgorithm` serves as a specialized framework for describing various forms of online learning algorithms within BrainScale.\n",
    "\n",
    "The algorithmic support provided by `brainscale.ETraceAlgorithm` is founded upon the three fundamental concepts previously introduced: `ETraceState`, `ETraceParam`, and `ETraceOp`. \n",
    "\n",
    "`brainscale.ETraceAlgorithm` implements a flexible online learning compiler that enables online learning capabilities for any neural network model constructed using these three foundational concepts."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76668ec1ab34eaeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Specifically, BrainScale currently supports the following online learning algorithms:\n",
    "\n",
    "1. `brainscale.DiagIODimAlgorithm`\n",
    "    - Implements the ES-D-RTRL algorithm for online learning\n",
    "    - Achieves $O(N)$ memory and computational complexity for online gradient computation\n",
    "    - Optimized for large-scale spiking neural network models\n",
    "    - Detailed algorithm specifications are available in [our paper](https://doi.org/10.1101/2024.09.24.614728)\n",
    "\n",
    "2. `brainscale.DiagParamDimAlgorithm`\n",
    "    - Utilizes the D-RTRL algorithm for online learning\n",
    "    - Features $O(N^2)$ memory and computational complexity for online gradient computation\n",
    "    - Applicable to both recurrent neural networks and spiking neural network models\n",
    "    - Complete algorithmic details are documented in [our paper](https://doi.org/10.1101/2024.09.24.614728)\n",
    "\n",
    "3. `brainscale.DiagHybridDimAlgorithm`\n",
    "    - Implements selective application of ES-D-RTRL or D-RTRL algorithms for parameter updates\n",
    "    - Preferentially employs D-RTRL for convolutional layers and highly sparse connections\n",
    "    - Optimizes memory and computational complexity of parameter updates through adaptive algorithm selection\n",
    "\n",
    "The framework is designed for extensibility, with ongoing development to support additional online learning algorithms for diverse application scenarios."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c49bb176c385eb1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following demonstration, we will illustrate the process of constructing a neural network model with online learning capabilities using `brainscale.ETraceAlgorithm`. This example will serve to exemplify the practical implementation of the concepts discussed above."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a6bd8f6a4fb35dd"
  },
  {
   "cell_type": "code",
   "source": [
    "with bst.environ.context(dt=0.1 * u.ms):\n",
    "\n",
    "    # Define a simple recurrent neural network composed of LIF neurons\n",
    "    model = LIF_Delta_Net(10, 10)\n",
    "    bst.nn.init_all_states(model)\n",
    "    \n",
    "    # The model is fed into an online learning algorithm with a view to online learning\n",
    "    model = brainscale.DiagIODimAlgorithm(model, decay_or_rank=0.99)\n",
    "    \n",
    "    # Compile the model's eligibility trace based on one input data. \n",
    "    # Thereafter, the model is called to update not only the state \n",
    "    # of the model, but also the model's eligibility trace\n",
    "    example_input = bst.random.random(10) < 0.1\n",
    "    model.compile_graph(example_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:10.510134Z",
     "start_time": "2024-11-24T07:25:09.890284Z"
    }
   },
   "id": "c39ae571c630da9c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "In essence, the user-defined neural network model solely specifies how the model states $\\mathbf{h}$ evolve forward in time as a function of inputs. The compiled `ETraceAlgorithm`, in contrast, defines the update dynamics of the model's eligibility traces $\\mathbf{\\epsilon}$ in relation to state updates. Consequently, subsequent model invocations result in concurrent updates of both model states and their corresponding eligibility traces."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f244553c077d8aba"
  },
  {
   "cell_type": "code",
   "source": [
    "with bst.environ.context(dt=0.1 * u.ms):\n",
    "    \n",
    "    out = model(example_input)\n",
    "\n",
    "# The eligibility trace of the model's pre-synaptic neural \n",
    "# activity trace can be obtained by calling \"model.etrace_xs\"\n",
    "bst.util.PrettyMapping(model.etrace_xs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:10.955275Z",
     "start_time": "2024-11-24T07:25:10.685300Z"
    }
   },
   "id": "4e050256b605e9a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  Var(id=2878074780864):float32[20]: ShortTermState(\n",
       "    value=Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.], dtype=float32)\n",
       "  )\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# The eligibility trace of the model's post-synaptic neural \n",
    "# activity trace can be obtained by calling \"model.etrace_dfs\"\n",
    "bst.util.PrettyMapping(model.etrace_dfs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:10.982009Z",
     "start_time": "2024-11-24T07:25:10.977142Z"
    }
   },
   "id": "d1c10f9de21c6938",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  (Var(id=2878074780928):float32[10], ('neu', 'V')): ShortTermState(\n",
       "    value=Array([0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],      dtype=float32)\n",
       "  )\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "`BrainScale` provides a comprehensive and elegant framework for online learning, with its core concepts organized into the following hierarchical layers:\n",
    "\n",
    "1. Infrastructure Layer\n",
    "    - Supports specific dynamical model architectures with strict separation between \"dynamics\" and \"interactions\"\n",
    "    - Built upon the `BrainState` ecosystem, maintaining full compatibility with its programming paradigm\n",
    "    - Provides ready-to-use neural network components through the `brainscale.nn` module\n",
    "\n",
    "2. Core Concepts Layer\n",
    "    - `ETraceState`: Designates hidden states requiring eligibility trace updates\n",
    "    - `ETraceParam`: Identifies model parameters requiring eligibility trace updates\n",
    "    - `ETraceOp`: Defines specific operations for dynamical interactions\n",
    "    - `ETraceParamOp`: Provides unified interface for parameter-operation combinations\n",
    "\n",
    "3. Algorithm Implementation Layer\n",
    "    - `DiagIODimAlgorithm`: Implements ES-D-RTRL algorithm with $O(N)$ complexity\n",
    "    - `DiagParamDimAlgorithm`: Implements D-RTRL algorithm with $O(N^2)$ complexity\n",
    "    - `DiagHybridDimAlgorithm`: Adaptive hybrid approach, selecting between $O(N)$ and $O(N^2)$ complexity algorithms based on network architecture\n",
    "\n",
    "4. Operational Workflow\n",
    "    - Construction of neural networks using foundational components\n",
    "    - Selection and application of appropriate online learning algorithms\n",
    "    - Model compilation generating eligibility trace computation graphs\n",
    "    - Concurrent updates of model states and eligibility traces through forward propagation\n",
    "\n",
    "BrainScale's distinctive architecture encapsulates complex online learning algorithms behind concise interfaces while providing flexible customization mechanisms. This design philosophy achieves a balance between:\n",
    "- High performance and usability\n",
    "- Seamless integration with the existing BrainState ecosystem\n",
    "- Intuitive and efficient construction of online learning neural networks\n",
    "\n",
    "Through this architectural approach, BrainScale transforms the development and training of online learning neural networks into an intuitive and efficient process, providing a powerful toolkit for neural computation research."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b7ab12e085b644b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
