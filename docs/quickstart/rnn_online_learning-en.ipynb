{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RNN Online Learning\n",
    "\n",
    "\n",
    "In the chapter on [Key Concepts](./concepts-en.ipynb), we introduced the fundamentals of online learning with `brainscale`. In this section, we will discuss how to implement online learning for Recurrent Neural Networks (RNNs) based on `brainscale`.\n",
    "\n",
    "Rate-based RNNs are more widely used in contemporary deep learning tasks compared to Spiking Neural Networks (SNNs). In these networks, the output of neurons is represented as continuous floating-point values, rather than discrete spikes as in SNNs. The `ParamDimVjpAlgorithm` provided by `brainscale` can be employed very efficiently to support online learning for RNNs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59500c56c6226022"
  },
  {
   "cell_type": "code",
   "source": [
    "import brainstate\n",
    "import brainscale"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:28:06.667925Z",
     "start_time": "2024-11-24T07:28:06.660935Z"
    }
   },
   "id": "d43067c8fa7ef9c0",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Basic Concepts of RNNs\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are specifically designed to process sequential data. They are widely used in fields such as natural language processing, time series prediction, and speech recognition.\n",
    "\n",
    "**Mathematical Model of RNNs**\n",
    "\n",
    "A key characteristic of RNNs is their ability to handle input sequences of arbitrary length and capture temporal dependencies within the sequences. The typical structure of an RNN is shown below:\n",
    "\n",
    "![RNNs](../_static/architecture-rnn-ltr.png)\n",
    "\n",
    "At each time step $ a^{<t>} $, the activation $ y^{<t>} $ is computed from the input $ x^{<t>} $ and the activation from the previous time step $ a^{<t-1>} $. The calculations for $ a^{<t>} $ and $ y^{<t>} $ are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{<t>} =& g_{1}(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_{a}) \\\\ \n",
    "y^{<t>} =& g_{2}(W_{ya}a^{<t>} + b_{y})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, $ \\mathcal{W}_{ax}, \\; \\mathcal{W}_{aa}, \\; \\mathcal{W}_{ya}, \\; b_{a}, \\; b_{y} $ are coefficients shared over time, and $ g_1 $ and $ g_2 $ are activation functions. During training, we need to update these parameters using gradient descent algorithms.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c07e5890f75e69f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Applications of RNNs**\n",
    "\n",
    "RNNs can be applied to a variety of tasks, including natural language processing (NLP), time series prediction, and speech recognition. The table below summarizes the different application domains:\n",
    "\n",
    "![RNNs Applications](../_static/rnn-applications.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be87008621376f2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training of RNNs**\n",
    "\n",
    "The loss function $\\mathcal{L}$ for RNNs is defined based on the loss at each time step, as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{L(\\hat{y},y)} = \\sum_{t=1}^{T_{y}} \\mathcal{L(\\hat{y}^{<t>},y^{<t>})}\n",
    "$$\n",
    "\n",
    "RNNs are typically trained using the backpropagation algorithm. At each time step, we compute the derivative of the loss $\\mathcal{L}$ with respect to the weight matrix $W$. At time step $T$, the derivative of the loss $\\mathcal{L}$ with respect to the weight matrix $W$ is expressed as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{L}}(T)}{\\partial W} = \\sum_{t=1}^{T} \\frac{\\partial{\\mathcal{L}}(T)}{\\partial W} \\Big|_{(t)}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae1e3a6e50ad6218"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Variants of RNNs**\n",
    "\n",
    "To address several core issues faced by traditional RNNs in processing sequential data—including gradient vanishing and explosion, control of information flow, contextual understanding, and the handling of complex data—researchers have proposed various variants of RNNs. The most popular among these include:\n",
    "\n",
    "1. **Long Short-Term Memory (LSTM)**: Proposed by Hochreiter and Schmidhuber in 1997, LSTM networks are designed to mitigate the gradient vanishing problem that traditional RNNs encounter when dealing with long-term dependencies.\n",
    "\n",
    "   An LSTM unit consists of three main components:\n",
    "\n",
    "   - **Forget Gate**: Determines how much information from the previous hidden state should be retained.\n",
    "   \n",
    "     $$\n",
    "     f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "     $$\n",
    "\n",
    "   - **Input Gate**: Controls the input of new information.\n",
    "   \n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     i_t =& \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\\\ \n",
    "     \\tilde{C}_t =& \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "   - **Output Gate**: Determines how much information from the current cell state should be output.\n",
    "   \n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     o_t =& \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\\\ \n",
    "     C_t =& f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t \\\\ \n",
    "     h_t =& o_t \\cdot \\tanh(C_t)\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "2. **Gated Recurrent Unit (GRU)**: A simplified version of LSTM introduced by Cho et al. in 2014. GRU retains the advantages of LSTM but with fewer parameters.\n",
    "\n",
    "   The GRU consists of two gates:\n",
    "\n",
    "   - **Reset Gate**: Controls the influence of the previous hidden state on the current state.\n",
    "   \n",
    "     $$\n",
    "     r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "     $$\n",
    "\n",
    "   - **Update Gate**: Determines the degree to which the current cell state is updated.\n",
    "   \n",
    "     $$\n",
    "     z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "     $$\n",
    "\n",
    "   - **Cell State Update**:\n",
    "   \n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     \\tilde{h}_t =& \\tanh(W \\cdot [r_t \\cdot h_{t-1}, x_t] + b) \\\\ \n",
    "     h_t =& (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h}_t\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n",
    "GRUs simplify the model by reducing the number of gates, allowing for faster training while achieving comparable performance to LSTMs on many tasks.\n",
    "\n",
    "It can be observed that this mathematical formulation of the RNN model perfectly satisfies the separation of \"dynamics\" and \"dynamic interactions\" as discussed in the [Key Concepts](./concepts-en.ipynb). Consequently, the online learning system provided by `brainscale` can effectively support online learning for RNN models.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79b631fb8038689e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. RNN Models Supported by `brainscale`\n",
    "\n",
    "`brainscale` does not support online learning for all RNN models. For instance, for the simplest Elman RNNs, the update rule is given by:\n",
    "\n",
    "$$\n",
    "h_t = f(W_hh_{t-1} + W_xx_t + b_h)\n",
    "$$\n",
    "\n",
    "`brainscale` does not facilitate online learning for this model. However, for more complex RNN models such as LSTM and GRU, `brainscale` is particularly well-suited for online learning. The primary reason is that the state updates in models like LSTM and GRU are implemented through gating mechanisms, which lead to rich intrinsic dynamics of state variables. As demonstrated in [our paper](https://doi.org/10.1101/2024.09.24.614728), the learning of temporal dependencies in `brainscale` is achieved through the dynamical updates of these element-wise operational state variables.\n",
    "\n",
    "Currently, we can utilize the `ParamDimVjpAlgorithm` for online learning of any RNN model. Let us illustrate how to use `brainscale` for online learning of RNNs with a simple example.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b989713fb51b12ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Online Learning of GRU Models Based on the Copying Task\n",
    "\n",
    "### 3.1 Copying Task Dataset\n",
    "\n",
    "The copying task is a classic benchmark used to test whether Recurrent Neural Networks (RNNs) can remember long-term dependencies. This task is widely employed to validate the performance of RNN models when faced with long sequences of information, particularly in assessing the model's ability to effectively retain input information while avoiding the impacts of gradient vanishing or explosion.\n",
    "\n",
    "In a typical copying task, the model's objective is to accurately reproduce a given input sequence to the output sequence after a certain delay. For example, suppose the input sequence has a length of \\( T \\):\n",
    "\n",
    "- The initial portion of the input sequence contains several specific symbols (such as letters or numbers), followed by a certain number of placeholders (e.g., \"0\") to delay the generation of the output.\n",
    "- The final part of the sequence includes a signal that instructs the model to start outputting the initial subset of symbols (i.e., the original input subsequence) at the next time steps.\n",
    "\n",
    "**Example**\n",
    "\n",
    "```bash\n",
    "Input sequence example:\n",
    "\"1 5 3 4 8 [wait symbol] ... [wait symbol] trigger symbol 0 0 0 0 0\"\n",
    "Expected output:\n",
    "\"0 0 0 0 0 0 ... 0 1 5 3 4 8\"\n",
    "```\n",
    "\n",
    "This sequence typically consists of three parts: (1) the numerical sequence that needs to be remembered (as in the example \"1 5 3 4 8\"), (2) the intermediate waiting period (filled with special symbols), and (3) the trigger symbol that prompts the output.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "780da5ce190fa33a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is a simple example of a data loader:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6879e52f3591637"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class CopyDataset:\n",
    "    def __init__(self, time_lag: int, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.seq_length = time_lag + 20\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            ids = np.zeros([self.batch_size, self.seq_length], dtype=int)\n",
    "            # Randomly generated 10 numbers\n",
    "            ids[..., :10] = np.random.randint(1, 9, (self.batch_size, 10))\n",
    "            # Add 10 placeholders to the last 10 bits of the input sequence\n",
    "            ids[..., -10:] = np.ones([self.batch_size, 10]) * 9\n",
    "            # input sequence\n",
    "            x = np.zeros([self.batch_size, self.seq_length, 10])\n",
    "            for i in range(self.batch_size):\n",
    "                x[i, range(self.seq_length), ids[i]] = 1\n",
    "            yield x, ids[..., :10]\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:28:06.723597Z",
     "start_time": "2024-11-24T07:28:06.719075Z"
    }
   },
   "id": "c8d286010f5333b9",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the code above, we define a `CopyDataset` class, which allows us to generate the copying task dataset using the `__iter__` method. In each iteration, we generate a sequence of length `time_lag + 20`, containing 10 random numbers and `time_lag` placeholders. Our objective is to replicate these 10 random numbers in the output sequence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d8842a1eb5e3b55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Definition of the GRU Model\n",
    "\n",
    "We use a GRU model to address the copying task. We can define the GRU model using the `GRUCell` provided by `brainscale`. Below is a simple example of a GRU model definition:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17c6379fc8c58af7"
  },
  {
   "cell_type": "code",
   "source": [
    "class GRUNet(brainstate.nn.Module):\n",
    "    def __init__(self, n_in, n_rec, n_out, n_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        # Building a GRU Multilayer Network\n",
    "        layers = []\n",
    "        for _ in range(n_layer):\n",
    "            layers.append(brainscale.nn.GRUCell(n_in, n_rec))\n",
    "            n_in = n_rec\n",
    "        self.layer = brainstate.nn.Sequential(*layers)\n",
    "\n",
    "        # Building the Output Layer\n",
    "        self.readout = brainscale.nn.Linear(n_rec, n_out)\n",
    "\n",
    "    def update(self, x):\n",
    "        return self.readout(self.layer(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:28:06.736507Z",
     "start_time": "2024-11-24T07:28:06.729936Z"
    }
   },
   "id": "aacdcd7061f05ba2",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Online Learning\n",
    "\n",
    "Next, we will first create an abstract `Trainer` class that enables model training for the copying task based on specified parameters. Then, we will implement two concrete trainers: `OnlineTrainer` and `BPTTTrainer`. The `OnlineTrainer` utilizes the online learning algorithm provided by `brainscale`, while the `BPTTTrainer` employs the Backpropagation Through Time (BPTT) algorithm for training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b07034be04ae57c4"
  },
  {
   "cell_type": "code",
   "source": [
    "import braintools as bts\n",
    "from tqdm import tqdm\n",
    "import jax\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: brainstate.nn.Module,\n",
    "        opt: brainstate.optim.Optimizer,\n",
    "        n_epochs: int,\n",
    "        n_seq: int,\n",
    "        batch_size: int = 128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # target network\n",
    "        self.target = target\n",
    "\n",
    "        # optimizer\n",
    "        self.opt = opt\n",
    "        weights = self.target.states().subset(brainstate.ParamState)\n",
    "        opt.register_trainable_weights(weights)\n",
    "\n",
    "        # training parameters\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_seq = n_seq\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def batch_train(self, xs, ys):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def f_train(self):\n",
    "        dataloader = CopyDataset(self.n_seq, self.batch_size)\n",
    "        bar = tqdm(enumerate(dataloader), total=self.n_epochs)\n",
    "        losses = []\n",
    "        for i, (x_local, y_local) in bar:\n",
    "            if i == self.n_epochs:\n",
    "                break\n",
    "            # training\n",
    "            x_local = np.transpose(x_local, (1, 0, 2))\n",
    "            y_local = np.transpose(y_local, (1, 0))\n",
    "            r = self.batch_train(x_local, y_local)\n",
    "            bar.set_description(f'Training {i:5d}, loss = {float(r):.5f}', refresh=True)\n",
    "            losses.append(r)\n",
    "        return np.asarray(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:28:06.748860Z",
     "start_time": "2024-11-24T07:28:06.742384Z"
    }
   },
   "id": "74fb4a80fe342cb1",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the code above, we define an abstract `Trainer` class that includes a `batch_train` method for training the model on a batch of data. We also define a `f_train` method, which serves as the direct entry point for model training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c7e2636a75d9609"
  },
  {
   "cell_type": "code",
   "source": [
    "class OnlineTrainer(Trainer):\n",
    "    @brainstate.compile.jit(static_argnums=(0,))\n",
    "    def batch_train(self, inputs, target):\n",
    "        weights = self.target.states(brainstate.ParamState)\n",
    "\n",
    "        # For each batch of data, reinitialise the model state\n",
    "        brainstate.nn.init_all_states(self.target, inputs.shape[1])\n",
    "\n",
    "        # Initialising an online learning model\n",
    "        # Here, we need to use mode to specify that the dataset to be used is one with batch dimensions\n",
    "        model = brainscale.ParamDimVjpAlgorithm(self.target, mode=brainstate.mixin.Batching())\n",
    "\n",
    "        # Using a sample data compilation for online learning eligibility trace\n",
    "        model.compile_graph(inputs[0])\n",
    "\n",
    "        def _etrace_loss(inp, tar):\n",
    "            # call the model\n",
    "            out = model(inp)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = bts.metric.softmax_cross_entropy_with_integer_labels(out, tar).mean()\n",
    "            return loss, out\n",
    "\n",
    "        def _etrace_grad(prev_grads, x):\n",
    "            inp, tar = x\n",
    "            # Calculate the gradient at the current moment\n",
    "            f_grad = brainstate.augment.grad(_etrace_loss, weights, has_aux=True, return_value=True)\n",
    "            cur_grads, local_loss, out = f_grad(inp, tar)\n",
    "            # accumulate the gradients\n",
    "            next_grads = jax.tree.map(lambda a, b: a + b, prev_grads, cur_grads)\n",
    "            # return the gradients and the output\n",
    "            return next_grads, (out, local_loss)\n",
    "\n",
    "        def _etrace_train(inputs_):\n",
    "            # initialize the gradients\n",
    "            grads = jax.tree.map(lambda a: jax.numpy.zeros_like(a), {k: v.value for k, v in weights.items()})\n",
    "            # calculate the gradients\n",
    "            grads, (outs, losses) = brainstate.compile.scan(_etrace_grad, grads, (inputs_, target))\n",
    "            # optimization\n",
    "            self.opt.update(grads)\n",
    "            return losses.mean()\n",
    "\n",
    "        # Prior to the moment T, the model updates its state and its eligibility trace\n",
    "        n_sim = self.n_seq + 10\n",
    "        brainstate.compile.for_loop(model, inputs[:n_sim])\n",
    "\n",
    "        # After moment T, the model starts learning online\n",
    "        r = _etrace_train(inputs[n_sim:])\n",
    "        return r\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:28:06.762134Z",
     "start_time": "2024-11-24T07:28:06.754027Z"
    }
   },
   "id": "9c50dd12b4fd6641",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "In the code above, we define an `OnlineTrainer` class that inherits from the `Trainer` class. Within this class, we utilize the `brainscale.ParamDimVjpAlgorithm` for online learning. At each time step, we compute the gradient of the loss function and use a gradient descent algorithm to update the model parameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b68477cf230cdf4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Offline Learning\n",
    "\n",
    "To compare the performance of online learning, we have implemented a `BPTTTrainer` class that utilizes the Backpropagation Through Time (BPTT) algorithm for training. Below is the implementation of the `BPTTTrainer` class:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8018f7ac46de4ee8"
  },
  {
   "cell_type": "code",
   "source": [
    "class BPTTTrainer(Trainer):\n",
    "    @brainstate.compile.jit(static_argnums=(0,))\n",
    "    def batch_train(self, inputs, targets):\n",
    "        # initialize the states\n",
    "        brainstate.nn.init_all_states(self.target, inputs.shape[1])\n",
    "\n",
    "        # weights\n",
    "        weights = self.target.states(brainstate.ParamState)\n",
    "\n",
    "        def _run_step_train(inp, tar):\n",
    "            out = self.target(inp)\n",
    "            loss = bts.metric.softmax_cross_entropy_with_integer_labels(out, tar).mean()\n",
    "            return out, loss\n",
    "\n",
    "        def _bptt_grad_step():\n",
    "            # Prior to the moment T, the model updates its state\n",
    "            n_sim = self.n_seq + 10\n",
    "            _ = brainstate.compile.for_loop(self.target, inputs[:n_sim])\n",
    "            # After moment T, the model starts learning offline\n",
    "            outs, losses = brainstate.compile.for_loop(_run_step_train, inputs[n_sim:], targets)\n",
    "            return losses.mean(), outs\n",
    "\n",
    "        # gradients\n",
    "        grads, loss, outs = brainstate.augment.grad(_bptt_grad_step, weights, has_aux=True, return_value=True)()\n",
    "\n",
    "        # optimization\n",
    "        self.opt.update(grads)\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:28:06.774100Z",
     "start_time": "2024-11-24T07:28:06.767451Z"
    }
   },
   "id": "c79aa026f9ff2129",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Comparison of Model Training: Online Learning vs. Offline Learning\n",
    "\n",
    "Next, we will compare the performance of online learning and offline learning. We will train the model for 100 epochs and record the loss value for each epoch. Each epoch consists of 128 samples, with each sample containing 200 sequences.\n",
    "\n",
    "Training the online learning model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15e36d056d52e255"
  },
  {
   "cell_type": "code",
   "source": [
    "online = OnlineTrainer(\n",
    "    target=GRUNet(10, 200, 10, 1),\n",
    "    opt=brainstate.optim.Adam(0.001),\n",
    "    n_epochs=1000,\n",
    "    n_seq=200,\n",
    "    batch_size=128,\n",
    ")\n",
    "online_losses = online.f_train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T07:56:31.047527Z",
     "start_time": "2024-11-24T07:28:06.779623Z"
    }
   },
   "id": "5cec4d700da7780f",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the BPTT model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd1bdf13f9e9f762"
  },
  {
   "cell_type": "code",
   "source": [
    "bptt = BPTTTrainer(\n",
    "    target=GRUNet(10, 200, 10, 1),\n",
    "    opt=brainstate.optim.Adam(0.001),\n",
    "    n_epochs=1000,\n",
    "    n_seq=200,\n",
    "    batch_size=128,\n",
    ")\n",
    "bptt_losses = bptt.f_train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T08:23:16.572571Z",
     "start_time": "2024-11-24T07:56:31.326370Z"
    }
   },
   "id": "4bcf2a285f066f6a",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparing training loss for online and offline learning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1177f352a4451880"
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(online_losses, label='Online Learning')\n",
    "plt.plot(bptt_losses, label='BPTT')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T08:23:16.787761Z",
     "start_time": "2024-11-24T08:23:16.602882Z"
    }
   },
   "id": "19c2eec5263a2598",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "This tutorial provides a comprehensive overview of how to implement online learning for RNNs using the `brainscale` framework. Notably, `brainscale` is particularly well-suited for complex RNN models with gating mechanisms, such as LSTM and GRU. In contrast, for RNN models like Vanilla RNNs, which rely solely on weight interactions to induce recurrence, the `brainscale` online learning system struggles to support accurate gradient computation.\n",
    "\n",
    "We hope this tutorial will assist researchers and engineers in effectively utilizing the `brainscale` online learning system to address challenges encountered in the practical training of RNNs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f6109865777367c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
