{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Batching Online Learning\n",
    "\n",
    "\n",
    "The online learning module of the BrainScale framework provides two efficient batching strategies to optimize neural network training processes:\n",
    "\n",
    "- **Manual Batching**: Explicitly manage batch dimensions with model state shape `(B, M)`, where `B` is the batch size and `M` is the number of model parameters\n",
    "- **Automatic Batching**: Use `brainstate.transform.vmap` function for vectorized operations, maintaining single-sample model state shape `(M)` while processing batch data through automatic vectorization\n",
    "\n",
    "**ðŸ’¡ Memory and Computational Advantages of Batching**\n",
    "\n",
    "**1. Memory Layout Optimization:**\n",
    "```python\n",
    "# Inefficient memory layout (sample-by-sample)\n",
    "for i in range(128):\n",
    "    process_sample(i)  # 128 memory allocations\n",
    "\n",
    "# Efficient memory layout (batching)\n",
    "process_batch(all_128_samples)  # 1 memory allocation, contiguous storage\n",
    "```\n",
    "\n",
    "**2. Parallel Computing Advantages:**\n",
    "```python\n",
    "# CPU sample-by-sample: 128 Ã— single sample time\n",
    "# GPU batching: approximately equal to single sample time (ideal case)\n",
    "```\n",
    "\n",
    "The core concept of manual batching is: **explicitly control batch dimensions to maximize parallel computing efficiency**. While the code is slightly more complex, it provides significant performance improvements in large-scale training.\n",
    "\n",
    "This tutorial will provide a detailed comparison of the implementation differences, applicable scenarios, and performance characteristics of these two approaches."
   ],
   "id": "bae2465a56866a92"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-22T05:56:17.985321Z",
     "start_time": "2025-07-22T05:56:15.026242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import brainstate\n",
    "import braintools\n",
    "import brainscale\n",
    "import brainunit as u\n",
    "import jax\n",
    "\n",
    "brainstate.environ.set(dt=1.0 * u.ms)\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preparation: Dataset + Model\n",
    "\n",
    "First, we create a simulated classification task dataset.\n",
    "\n"
   ],
   "id": "e5e67c636a3c81f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:56:18.487311Z",
     "start_time": "2025-07-22T05:56:18.047612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset parameter configuration\n",
    "n_time = 16  # Time steps\n",
    "n_batch = 128      # Batch size\n",
    "n_in = 100         # Input feature dimension\n",
    "n_hidden = 200     # Hidden layer neuron count\n",
    "n_out = 10         # Output class count\n",
    "\n",
    "# Generate random training data\n",
    "xs = brainstate.random.rand(n_time, n_batch, n_in)  # Input data shape: (16, 128, 100)\n",
    "ys = brainstate.random.randint(0, n_out, n_batch)   # Label data shape: (128,)\n"
   ],
   "id": "90728ce10b56cbf8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we construct a recurrent neural network based on Leaky Integrate-and-Fire (LIF) neurons to perform classification on this dataset.\n",
   "id": "1bbeb1b46b611b89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:56:18.511103Z",
     "start_time": "2025-07-22T05:56:18.505888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LIFNet(brainstate.nn.Module):\n",
    "    \"\"\"\n",
    "    LIF Neural Network Model\n",
    "\n",
    "    Architecture: Input layer -> LIF neuron layer (with recurrent connections) -> Output layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "\n",
    "        # LIF neuron layer: simulates biological neuron leaky integrate-and-fire behavior\n",
    "        self.neu = brainscale.nn.LIF(n_hidden)\n",
    "\n",
    "        # Weight initialization strategy\n",
    "        rec_init = brainstate.init.KaimingNormal(unit=u.mV)    # Recurrent connection weights\n",
    "        ff_init = brainstate.init.KaimingNormal(unit=u.mV)     # Feedforward connection weights\n",
    "\n",
    "        # Synaptic connection layer: integrates feedforward input and recurrent feedback\n",
    "        self.syn = brainstate.nn.DeltaProj(\n",
    "            comm=brainscale.nn.Linear(\n",
    "                n_in + n_hidden, n_hidden,\n",
    "                # Connection weight matrix: [feedforward weights; recurrent weights]\n",
    "                w_init=u.math.concatenate([\n",
    "                    ff_init([n_in, n_hidden]),\n",
    "                    rec_init([n_hidden, n_hidden])\n",
    "                ], axis=0),\n",
    "                b_init=brainstate.init.ZeroInit(unit=u.mV)\n",
    "            ),\n",
    "            post=self.neu\n",
    "        )\n",
    "\n",
    "        # Output layer: converts spike activity to classification output\n",
    "        self.out = brainstate.nn.LeakyRateReadout(n_hidden, n_out)\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        Model forward propagation\n",
    "\n",
    "        Args:\n",
    "            x: Input data\n",
    "\n",
    "        Returns:\n",
    "            Network output (classification logits)\n",
    "        \"\"\"\n",
    "        # Integrate current input and recurrent spike feedback\n",
    "        combined_input = u.math.concatenate([x, self.neu.get_spike()], axis=-1)\n",
    "        self.syn(combined_input)\n",
    "\n",
    "        # Return current timestep output\n",
    "        return self.out(self.neu())\n"
   ],
   "id": "da25cad55d2fd661",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Manual Batching\n",
    "\n",
    "Manual batching requires the following conditions:\n",
    "\n",
    "1. Initialize the model with batch model states, where model state shape is $\\mathbb{R}^{BÃ—M}$, with $B$ being the batch size and $M$ being the number of model parameters.\n",
    "2. When calling the model's `.update` function, pass a batch of sample data with shape $\\mathbb{R}^{BÃ—D}$, where $D$ is the sample data dimension.\n",
    "3. When initializing the online learning algorithm, set the `mode` parameter to `brainstate.mixin.Batching()` to enable manual batching mode. Alternatively, use `brainstate.environ.set(mode=brainstate.mixin.Batching())` to set global batching mode.\n",
    "\n",
    "### Core Features\n",
    "\n",
    "Manual batching mode requires developers to explicitly handle batch dimensions:\n",
    "\n",
    "1. **State Initialization**: Model state shape must be `(B, M)`\n",
    "2. **Data Format**: Input data shape is `(B, D)`\n",
    "3. **Mode Setting**: Use `brainstate.mixin.Batching()` to enable batching mode\n",
    "\n",
    "### Specific Example\n",
    "\n",
    "Here's a simple example of manual batching.\n",
    "\n"
   ],
   "id": "a2e103b1d7ea7691"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:56:18.532270Z",
     "start_time": "2025-07-22T05:56:18.524079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainerManualBatching:\n",
    "    \"\"\"Manual Batching Trainer\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        self.model = LIFNet(n_in, n_hidden, n_out)\n",
    "        self.optimizer = brainstate.optim.Adam(lr=1e-3)\n",
    "        # Register trainable parameters\n",
    "        self.optimizer.register_trainable_weights(self.model.states(brainstate.ParamState))\n",
    "\n",
    "    @brainstate.transform.jit(static_argnums=0)\n",
    "    def train(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Single training step\n",
    "\n",
    "        Args:\n",
    "            inputs: Input sequence shape: (T, B, D)\n",
    "            targets: Target labels shape: (B,)\n",
    "        \"\"\"\n",
    "        # Step 1: Initialize batch model states\n",
    "        brainstate.nn.init_all_states(self.model, batch_size=inputs.shape[1])\n",
    "\n",
    "        # Step 2: Create online learning algorithm instance\n",
    "        model = brainscale.ES_D_RTRL(\n",
    "            self.model,\n",
    "            decay_or_rank=0.9,                           # Eligibility trace decay factor\n",
    "            mode=brainstate.mixin.Batching()            # Enable manual batching mode\n",
    "        )\n",
    "\n",
    "        # Step 3: Compile computation graph (optimize execution efficiency)\n",
    "        model.compile_graph(inputs[0])\n",
    "\n",
    "        # Step 4: Get trainable parameters\n",
    "        weights = self.model.states(brainstate.ParamState)\n",
    "\n",
    "        def _etrace_grad(inp):\n",
    "            \"\"\"Calculate single-step loss and gradients\"\"\"\n",
    "            out = model(inp)\n",
    "            loss = braintools.metric.softmax_cross_entropy_with_integer_labels(\n",
    "                out, targets\n",
    "            ).mean()\n",
    "            return loss, out\n",
    "\n",
    "        def _etrace_step(prev_grads, x):\n",
    "            \"\"\"Eligibility trace gradient accumulation step\"\"\"\n",
    "            # Calculate current step gradients\n",
    "            f_grad = brainstate.augment.grad(\n",
    "                _etrace_grad, weights,\n",
    "                has_aux=True, return_value=True\n",
    "            )\n",
    "            cur_grads, local_loss, out = f_grad(x)\n",
    "\n",
    "            # Accumulate gradients (eligibility trace mechanism)\n",
    "            next_grads = jax.tree.map(lambda a, b: a + b, prev_grads, cur_grads)\n",
    "            return next_grads, (out, local_loss)\n",
    "\n",
    "        # Step 5: Temporal forward propagation and gradient accumulation\n",
    "        grads = jax.tree.map(u.math.zeros_like, weights.to_dict_values())\n",
    "        grads, (outs, losses) = brainstate.compile.scan(_etrace_step, grads, inputs)\n",
    "\n",
    "        # Step 6: Gradient clipping and parameter update\n",
    "        grads = brainstate.functional.clip_grad_norm(grads, 1.0)\n",
    "        self.optimizer.update(grads)\n",
    "\n",
    "        return losses.mean()\n",
    "\n",
    "    def f_train(self, n_epochs, inputs, targets):\n",
    "        \"\"\"Complete training process\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self.train(inputs, targets)\n",
    "            print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}')"
   ],
   "id": "a1d636a49ce053c8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:56:19.949893Z",
     "start_time": "2025-07-22T05:56:18.560859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create trainer and start training\n",
    "trainer_manual = TrainerManualBatching(n_in, n_hidden, n_out)\n",
    "trainer_manual.f_train(10, xs, ys)"
   ],
   "id": "66df14080e6686d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.8891\n",
      "Epoch 2/10, Loss: 3.3748\n",
      "Epoch 3/10, Loss: 2.9881\n",
      "Epoch 4/10, Loss: 2.7006\n",
      "Epoch 5/10, Loss: 2.7921\n",
      "Epoch 6/10, Loss: 2.7267\n",
      "Epoch 7/10, Loss: 2.5467\n",
      "Epoch 8/10, Loss: 2.5019\n",
      "Epoch 9/10, Loss: 2.4054\n",
      "Epoch 10/10, Loss: 2.3258\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Code Explanation\n",
    "\n",
    "**Data Flow Overview**\n",
    "\n",
    "Before diving into the analysis, let's understand the entire batching data flow:\n",
    "\n",
    "```\n",
    "Input data: inputs(T, B, D) â†’ Model states(B, M) â†’ Batch computation â†’ Gradient accumulation â†’ Parameter update\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `T`: Number of time steps\n",
    "- `B`: Batch size\n",
    "- `D`: Input feature dimension\n",
    "- `M`: Model state dimension\n",
    "\n",
    "**ðŸ”‘ Three Core Elements of Batch Operations**\n",
    "\n",
    "**1. Batch State Initialization - Why is this critical?**\n",
    "\n",
    "```python\n",
    "brainstate.nn.init_all_states(self.model, batch_size=inputs.shape[1])\n",
    "```\n",
    "\n",
    "What this line of code actually does:\n",
    "\n",
    "```python\n",
    "# Original state (single sample)\n",
    "Neuron voltage: V â†’ shape (200,)\n",
    "Neuron spike: spike â†’ shape (200,)\n",
    "\n",
    "# After batching (128 samples)\n",
    "Neuron voltage: V â†’ shape (128, 200)\n",
    "Neuron spike: spike â†’ shape (128, 200)\n",
    "```\n",
    "\n",
    "Why is this necessary?\n",
    "\n",
    "- RNN/SNN needs to maintain states between time steps\n",
    "- Batching means simultaneously processing 128 independent sequences\n",
    "- Each sequence needs its own state copy\n",
    "\n",
    "**2. Batching Mode - How does the algorithm perceive batching?**\n",
    "\n",
    "```python\n",
    "mode=brainstate.mixin.Batching()\n",
    "```\n",
    "\n",
    "This parameter tells the ES_D_RTRL algorithm:\n",
    "\n",
    "```python\n",
    "# Without Batching mode expectation\n",
    "Input: (100,) single sample\n",
    "Output: (10,) single prediction\n",
    "\n",
    "# With Batching mode expectation\n",
    "Input: (128, 100) batch samples\n",
    "Output: (128, 10) batch predictions\n",
    "```\n",
    "\n",
    "**3. Temporal Batch Processing - The most complex part**\n",
    "\n",
    "```python\n",
    "grads, (outs, losses) = brainstate.compile.scan(_etrace_step, grads, inputs)\n",
    "```\n",
    "\n",
    "What happens here:\n",
    "\n",
    "```python\n",
    "# inputs shape: (50, 128, 100) - 50 time steps, 128 samples per step\n",
    "\n",
    "Time step 0: Process inputs[0] â†’ (128, 100) â†’ Update 128 states â†’ Calculate gradients\n",
    "Time step 1: Process inputs[1] â†’ (128, 100) â†’ Update 128 states â†’ Accumulate gradients\n",
    "...\n",
    "Time step 49: Process inputs[49] â†’ (128, 100) â†’ Update 128 states â†’ Final gradients\n",
    "```\n"
   ],
   "id": "372664fc78cd11f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Automatic Batching\n",
    "\n",
    "Automatic batching primarily uses the `brainstate.transform.vmap` function for implementation. This function can vectorize the model's update function to achieve batch operations.\n",
    "\n",
    "### Core Features\n",
    "\n",
    "Automatic batching implements vectorized operations through the `vmap` function:\n",
    "\n",
    "1. **State Management**: Model states maintain single-sample shape `(M)`\n",
    "2. **Automatic Vectorization**: `vmap` automatically handles batch dimension mapping\n",
    "3. **Code Simplicity**: Reduces complexity of manual batching\n",
    "\n",
    "### Specific Example\n"
   ],
   "id": "b2f15bf1437e755f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:56:19.972154Z",
     "start_time": "2025-07-22T05:56:19.962961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainerAutoBatching:\n",
    "    \"\"\"Automatic Batching Trainer\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        self.model = LIFNet(n_in, n_hidden, n_out)\n",
    "        self.optimizer = brainstate.optim.Adam(lr=1e-3)\n",
    "        self.optimizer.register_trainable_weights(self.model.states(brainstate.ParamState))\n",
    "\n",
    "    @brainstate.transform.jit(static_argnums=0)\n",
    "    def train(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Single training step (automatic batching version)\n",
    "\n",
    "        Args:\n",
    "            inputs: Input sequence shape: (T, B, D)\n",
    "            targets: Target labels shape: (B,)\n",
    "        \"\"\"\n",
    "        # Step 1: Create online learning algorithm instance (no manual batching mode needed)\n",
    "        model = brainscale.ES_D_RTRL(self.model, decay_or_rank=0.9)\n",
    "\n",
    "        # Step 2: Use vmap to create batch state initialization function\n",
    "        @brainstate.transform.vmap_new_states(\n",
    "            axis_size=inputs.shape[1],                   # Batch size\n",
    "            state_tag='new',                            # State tag (for distinguishing different state groups)\n",
    "        )\n",
    "        def init():\n",
    "            \"\"\"Initialize single sample model states\"\"\"\n",
    "            brainstate.nn.init_all_states(self.model)\n",
    "            model.compile_graph(inputs[0, 0])           # Compile graph with single sample\n",
    "\n",
    "        # Execute batch initialization\n",
    "        init()\n",
    "\n",
    "        # Step 3: Create vectorized model wrapper\n",
    "        vmap_model = brainstate.nn.Vmap(\n",
    "            model,\n",
    "            vmap_states='new'                           # Specify state group to vectorize\n",
    "        )\n",
    "\n",
    "        # Step 4: Get trainable parameters\n",
    "        weights = self.model.states(brainstate.ParamState)\n",
    "\n",
    "        def _etrace_grad(inp):\n",
    "            \"\"\"Calculate single-step loss and gradients (auto-vectorized version)\"\"\"\n",
    "            out = vmap_model(inp)                       # Automatically handle batch dimensions\n",
    "            loss = braintools.metric.softmax_cross_entropy_with_integer_labels(\n",
    "                out, targets\n",
    "            ).mean()\n",
    "            return loss, out\n",
    "\n",
    "        def _etrace_step(prev_grads, x):\n",
    "            \"\"\"Eligibility trace gradient accumulation step\"\"\"\n",
    "            f_grad = brainstate.augment.grad(\n",
    "                _etrace_grad, weights,\n",
    "                has_aux=True, return_value=True\n",
    "            )\n",
    "            cur_grads, local_loss, out = f_grad(x)\n",
    "            next_grads = jax.tree.map(lambda a, b: a + b, prev_grads, cur_grads)\n",
    "            return next_grads, (out, local_loss)\n",
    "\n",
    "        # Step 5: Temporal forward propagation and gradient accumulation\n",
    "        grads = jax.tree.map(u.math.zeros_like, weights.to_dict_values())\n",
    "        grads, (outs, losses) = brainstate.compile.scan(_etrace_step, grads, inputs)\n",
    "\n",
    "        # Step 6: Gradient clipping and parameter update\n",
    "        grads = brainstate.functional.clip_grad_norm(grads, 1.0)\n",
    "        self.optimizer.update(grads)\n",
    "\n",
    "        return losses.mean()\n",
    "\n",
    "    def f_train(self, n_epochs, inputs, targets):\n",
    "        \"\"\"Complete training process\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self.train(inputs, targets)\n",
    "            print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}')\n"
   ],
   "id": "dcbc384f518020dd",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:56:20.953099Z",
     "start_time": "2025-07-22T05:56:19.983608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create trainer and start training\n",
    "trainer_auto = TrainerAutoBatching(n_in, n_hidden, n_out)\n",
    "trainer_auto.f_train(10, xs, ys)"
   ],
   "id": "25339a8eb842e453",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.0168\n",
      "Epoch 2/10, Loss: 3.1031\n",
      "Epoch 3/10, Loss: 2.8214\n",
      "Epoch 4/10, Loss: 2.5791\n",
      "Epoch 5/10, Loss: 2.6134\n",
      "Epoch 6/10, Loss: 2.5754\n",
      "Epoch 7/10, Loss: 2.4770\n",
      "Epoch 8/10, Loss: 2.3772\n",
      "Epoch 9/10, Loss: 2.3254\n",
      "Epoch 10/10, Loss: 2.2870\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Code Explanation\n",
    "\n",
    "**Core Concept Comparison**\n",
    "\n",
    "Before diving into analysis, let's understand the fundamental differences between automatic and manual batching:\n",
    "\n",
    "| Dimension | Manual Batching | Automatic Batching |\n",
    "|-----------|-----------------|-------------------|\n",
    "| **State Management** | Explicit batch states `(B, M)` | Single-sample states `(M)` + auto-vectorization |\n",
    "| **Computation Method** | Direct batch computation | `vmap` functional mapping |\n",
    "| **Code Complexity** | Need to handle batch dimensions | Abstract away batch details |\n",
    "\n",
    "```python\n",
    "# Conceptual comparison\n",
    "Manual batching approach: Create 128 neuron state copies, compute simultaneously\n",
    "Automatic batching approach: Define single neuron computation logic, auto-replicate 128 times\n",
    "```\n",
    "\n",
    "**Detailed Analysis of Key Steps**\n",
    "\n",
    "**Step 1: Algorithm Instance Creation (Simplified Mode)**\n",
    "\n",
    "```python\n",
    "# Step 1: Create online learning algorithm instance (no manual batching mode needed)\n",
    "model = brainscale.ES_D_RTRL(self.model, decay_or_rank=0.9)\n",
    "```\n",
    "\n",
    "**Automatic Batching Key Point:**\n",
    "\n",
    "Notice here we do **NOT** use `mode=brainstate.mixin.Batching()`:\n",
    "\n",
    "```python\n",
    "# Manual batching version\n",
    "model = brainscale.ES_D_RTRL(\n",
    "    self.model,\n",
    "    decay_or_rank=0.9,\n",
    "    mode=brainstate.mixin.Batching()  # Explicitly enable batching\n",
    ")\n",
    "\n",
    "# Automatic batching version\n",
    "model = brainscale.ES_D_RTRL(self.model, decay_or_rank=0.9)\n",
    "# Algorithm thinks it's processing a single sample!\n",
    "```\n",
    "\n",
    "**Step 2: vmap State Initialization (Core Mechanism) - The Magic of State Vectorization**\n",
    "\n",
    "```python\n",
    "# Step 2: Use vmap to create batch state initialization function\n",
    "@brainstate.transform.vmap_new_states(\n",
    "    axis_size=inputs.shape[1],                   # Batch size\n",
    "    state_tag='new',                            # State tag (for distinguishing different state groups)\n",
    ")\n",
    "def init():\n",
    "    \"\"\"Initialize single sample model states\"\"\"\n",
    "    brainstate.nn.init_all_states(self.model) # Only initialize 1!\n",
    "    model.compile_graph(inputs[0, 0])           # Compile graph with single sample\n",
    "\n",
    "# Execute batch initialization\n",
    "init()\n",
    "```\n",
    "\n",
    "Key understanding here:\n",
    "- The function `init()` only knows how to initialize **1 sample's** states\n",
    "- `vmap_new_states` automatically **replicates this function 128 times**\n",
    "- Result: Get 128 independent state copies, but code only wrote logic for 1 sample\n",
    "- State tag `'new'` helps distinguish different state groups, ensuring convenient extraction of these batch-initialized states later\n",
    "\n",
    "**Step 3: Vmap Model Wrapper - Automatic Single Sampleâ†’Batch Conversion**\n",
    "\n",
    "```python\n",
    "vmap_model = brainstate.nn.Vmap(model, vmap_states='new')\n",
    "\n",
    "# Internal flow when called:\n",
    "Input: (128, 100)\n",
    "  â†“ vmap auto-decomposes\n",
    "128 parallel computations: each processes (100,) â†’ (10,)\n",
    "  â†“ vmap auto-combines\n",
    "Output: (128, 10)\n",
    "```\n",
    "\n",
    "When you call `vmap_model(inp)`:\n",
    "\n",
    "```python\n",
    "# inp.shape = (128, 100)\n",
    "\n",
    "# Step 1: vmap decomposes input\n",
    "sample_0 = inp[0]   # (100,)\n",
    "sample_1 = inp[1]   # (100,)\n",
    "...\n",
    "sample_127 = inp[127] # (100,)\n",
    "\n",
    "# Step 2: Parallel execution (conceptually, may actually be vectorized)\n",
    "result_0 = model_with_state_0(sample_0)   # (10,)\n",
    "result_1 = model_with_state_1(sample_1)   # (10,)\n",
    "...\n",
    "result_127 = model_with_state_127(sample_127) # (10,)\n",
    "\n",
    "# Step 3: vmap combines output\n",
    "output = stack([result_0, result_1, ..., result_127])  # (128, 10)\n",
    "```\n",
    "\n",
    "**Key Insight:** The model function always thinks it's processing a single sample, completely unaware of batching!\n",
    "\n",
    "The beauty of this design is that you can write neural network logic in the most simple and intuitive way (single sample), then automatically gain efficient batching capabilities.\n",
    "\n"
   ],
   "id": "7e85bb6f4d261b9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "BrainScale's two batching strategies each have their advantages. Manual batching provides finer control and higher performance, suitable for production environments with large-scale training; automatic batching reduces implementation complexity with a cleaner API, making it more suitable for research and prototyping.\n",
    "\n",
    "Choosing the appropriate batching strategy requires comprehensive consideration of specific application scenarios, performance requirements, and development efficiency. It's recommended to use automatic batching in early project stages for rapid idea validation, and consider migrating to manual batching during performance optimization phases."
   ],
   "id": "a9778c65bc846112"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
