{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 在线学习的批处理操作\n",
    "\n",
    "BrainScale 框架的在线学习模块提供了两种高效的批处理策略，用于优化神经网络的训练过程：\n",
    "\n",
    "- **手动批处理**：显式管理批量维度，模型状态形状为 `(B, M)`，其中 `B` 为批处理大小，`M` 为模型参数数量\n",
    "- **自动批处理**：使用 `brainstate.transform.vmap` 函数实现向量化操作，模型状态保持单样本形状 `(M)`，通过自动向量化处理批量数据\n",
    "\n",
    "\n",
    "**💡 批处理的内存和计算优势**\n",
    "\n",
    "**1. 内存布局优化:**\n",
    "```python\n",
    "# 低效的内存布局（逐样本）\n",
    "for i in range(128):\n",
    "    process_sample(i)  # 128次内存分配\n",
    "\n",
    "# 高效的内存布局（批处理）\n",
    "process_batch(all_128_samples)  # 1次内存分配，连续存储\n",
    "```\n",
    "\n",
    "**2. 并行计算优势:**\n",
    "```python\n",
    "# CPU逐样本: 128 × 单样本时间\n",
    "# GPU批处理: 约等于 单样本时间（理想情况下）\n",
    "```\n",
    "\n",
    "这种手动批处理方法的核心思想是：**显式控制批量维度，最大化并行计算效率**。虽然代码稍微复杂，但在大规模训练中能带来显著的性能提升。\n",
    "\n",
    "本教程将详细对比这两种方法的实现差异、适用场景和性能特点。\n"
   ],
   "id": "68e3b7920d8038d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:57:29.419035Z",
     "start_time": "2025-07-22T05:57:26.507570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import brainstate\n",
    "import braintools\n",
    "import brainscale\n",
    "import brainunit as u\n",
    "import jax\n",
    "\n",
    "brainstate.environ.set(dt=1.0 * u.ms)  # 设置时间步长为1毫秒"
   ],
   "id": "70b482be16bccf95",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 准备工作：数据集+模型\n",
    "\n",
    "首先，我们创建一个模拟的分类任务数据集。"
   ],
   "id": "d540fa905d12ebc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:57:29.933591Z",
     "start_time": "2025-07-22T05:57:29.483010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据集参数配置\n",
    "n_time = 16  # Time steps\n",
    "n_batch = 128      # 批处理大小\n",
    "n_in = 100         # 输入特征维度\n",
    "n_hidden = 200     # 隐藏层神经元数量\n",
    "n_out = 10         # 输出类别数量\n",
    "\n",
    "# 生成随机训练数据\n",
    "xs = brainstate.random.rand(n_time, n_batch, n_in)  # Input data shape: (16, 128, 100)\n",
    "ys = brainstate.random.randint(0, n_out, n_batch)   # 标签数据 shape: (128,)"
   ],
   "id": "544c441ce7facf9c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "其次，构建一个基于漏积分发放（Leaky Integrate-and-Fire）神经元的循环神经网，用于完成这个数据集的分类任务。",
   "id": "20477d24cbc99616"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:57:29.966341Z",
     "start_time": "2025-07-22T05:57:29.959534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LIFNet(brainstate.nn.Module):\n",
    "    \"\"\"\n",
    "    LIF神经网络模型\n",
    "\n",
    "    结构：输入层 -> LIF神经元层（带循环连接）-> 输出层\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "\n",
    "        # LIF神经元层：模拟生物神经元的漏积分发放行为\n",
    "        self.neu = brainscale.nn.LIF(n_hidden)\n",
    "\n",
    "        # 权重初始化策略\n",
    "        rec_init = brainstate.init.KaimingNormal(unit=u.mV)    # 循环连接权重\n",
    "        ff_init = brainstate.init.KaimingNormal(unit=u.mV)     # 前馈连接权重\n",
    "\n",
    "        # 突触连接层：整合前馈输入和循环反馈\n",
    "        self.syn = brainstate.nn.DeltaProj(\n",
    "            comm=brainscale.nn.Linear(\n",
    "                n_in + n_hidden, n_hidden,\n",
    "                # 连接权重矩阵：[前馈权重; 循环权重]\n",
    "                w_init=u.math.concatenate([\n",
    "                    ff_init([n_in, n_hidden]),\n",
    "                    rec_init([n_hidden, n_hidden])\n",
    "                ], axis=0),\n",
    "                b_init=brainstate.init.ZeroInit(unit=u.mV)\n",
    "            ),\n",
    "            post=self.neu\n",
    "        )\n",
    "\n",
    "        # 输出层：将脉冲活动转换为分类输出\n",
    "        self.out = brainstate.nn.LeakyRateReadout(n_hidden, n_out)\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        模型前向传播\n",
    "\n",
    "        Args:\n",
    "            x: 输入数据\n",
    "\n",
    "        Returns:\n",
    "            网络输出（分类logits）\n",
    "        \"\"\"\n",
    "        # 整合当前输入和循环连接的脉冲反馈\n",
    "        combined_input = u.math.concatenate([x, self.neu.get_spike()], axis=-1)\n",
    "        self.syn(combined_input)\n",
    "\n",
    "        # 返回当前时刻的输出\n",
    "        return self.out(self.neu())"
   ],
   "id": "dcdc8b26beb52dc9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 手动批处理\n",
    "\n",
    "手动批处理需要如下几个要求：\n",
    "\n",
    "1. 初始化模型为一个批次的模型状态，模型状态的形状为$\\mathbb{R}^{B\\times M}$，其中$B$是批处理大小，$M$是模型参数的数量。\n",
    "2. 在每次调用模型的`.update`函数时，传入一个批次的样本数据，该数据的形状为$\\mathbb{R}^{B\\times D}$，其中$D$是样本数据的维度。\n",
    "3. 初始化在线学习算法时，将``mode``参数设置为`brainstate.mixin.Batching()`，以启用手动批处理模式。或者使用``brainstate.environ.set(mode=brainstate.mixin.Batching())``来设置全局批处理模式。\n",
    "\n",
    "### 核心特点\n",
    "\n",
    "手动批处理模式要求开发者显式处理批量维度：\n",
    "\n",
    "1. **状态初始化**：模型状态形状必须为 `(B, M)`\n",
    "2. **数据格式**：输入数据形状为 `(B, D)`\n",
    "3. **模式设置**：使用 `brainstate.mixin.Batching()` 启用批处理模式\n",
    "\n",
    "\n",
    "### 具体示例\n",
    "\n",
    "以下是一个简单的手动批处理的示例。"
   ],
   "id": "717848e9d05168ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:57:29.995137Z",
     "start_time": "2025-07-22T05:57:29.987752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainerManualBatching:\n",
    "    \"\"\"手动批处理训练器\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        self.model = LIFNet(n_in, n_hidden, n_out)\n",
    "        self.optimizer = brainstate.optim.Adam(lr=1e-3)\n",
    "        # 注册可训练参数\n",
    "        self.optimizer.register_trainable_weights(self.model.states(brainstate.ParamState))\n",
    "\n",
    "    @brainstate.transform.jit(static_argnums=0)\n",
    "    def train(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        单轮训练步骤\n",
    "\n",
    "        Args:\n",
    "            inputs: 输入序列 shape: (T, B, D)\n",
    "            targets: 目标标签 shape: (B,)\n",
    "        \"\"\"\n",
    "        # 步骤1：初始化批量模型状态\n",
    "        brainstate.nn.init_all_states(self.model, batch_size=inputs.shape[1])\n",
    "\n",
    "        # 步骤2：创建在线学习算法实例\n",
    "        model = brainscale.ES_D_RTRL(\n",
    "            self.model,\n",
    "            decay_or_rank=0.9,                           # 资格轨迹衰减因子\n",
    "            mode=brainstate.mixin.Batching()            # 启用手动批处理模式\n",
    "        )\n",
    "\n",
    "        # 步骤3：编译计算图（优化执行效率）\n",
    "        model.compile_graph(inputs[0])\n",
    "\n",
    "        # 步骤4：获取可训练参数\n",
    "        weights = self.model.states(brainstate.ParamState)\n",
    "\n",
    "        def _etrace_grad(inp):\n",
    "            \"\"\"计算单步的损失和梯度\"\"\"\n",
    "            out = model(inp)\n",
    "            loss = braintools.metric.softmax_cross_entropy_with_integer_labels(\n",
    "                out, targets\n",
    "            ).mean()\n",
    "            return loss, out\n",
    "\n",
    "        def _etrace_step(prev_grads, x):\n",
    "            \"\"\"资格轨迹梯度累积步骤\"\"\"\n",
    "            # 计算当前步的梯度\n",
    "            f_grad = brainstate.augment.grad(\n",
    "                _etrace_grad, weights,\n",
    "                has_aux=True, return_value=True\n",
    "            )\n",
    "            cur_grads, local_loss, out = f_grad(x)\n",
    "\n",
    "            # 累积梯度（资格轨迹机制）\n",
    "            next_grads = jax.tree.map(lambda a, b: a + b, prev_grads, cur_grads)\n",
    "            return next_grads, (out, local_loss)\n",
    "\n",
    "        # 步骤5：时序前向传播与梯度累积\n",
    "        grads = jax.tree.map(u.math.zeros_like, weights.to_dict_values())\n",
    "        grads, (outs, losses) = brainstate.compile.scan(_etrace_step, grads, inputs)\n",
    "\n",
    "        # 步骤6：梯度裁剪与参数更新\n",
    "        grads = brainstate.functional.clip_grad_norm(grads, 1.0)\n",
    "        self.optimizer.update(grads)\n",
    "\n",
    "        return losses.mean()\n",
    "\n",
    "    def f_train(self, n_epochs, inputs, targets):\n",
    "        \"\"\"完整训练流程\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self.train(inputs, targets)\n",
    "            print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}')"
   ],
   "id": "7528057359f346d4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:57:31.376941Z",
     "start_time": "2025-07-22T05:57:30.009201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建训练器并开始训练\n",
    "trainer_manual = TrainerManualBatching(n_in, n_hidden, n_out)\n",
    "trainer_manual.f_train(10, xs, ys)"
   ],
   "id": "c20114b63d0429d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.2803\n",
      "Epoch 2/10, Loss: 3.4519\n",
      "Epoch 3/10, Loss: 3.0564\n",
      "Epoch 4/10, Loss: 2.7560\n",
      "Epoch 5/10, Loss: 2.7972\n",
      "Epoch 6/10, Loss: 2.7649\n",
      "Epoch 7/10, Loss: 2.6040\n",
      "Epoch 8/10, Loss: 2.5192\n",
      "Epoch 9/10, Loss: 2.4659\n",
      "Epoch 10/10, Loss: 2.3979\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 代码详解\n",
    "\n",
    "**数据流概览**\n",
    "\n",
    "在深入分析之前，让我们先了解整个批处理的数据流：\n",
    "\n",
    "```\n",
    "输入数据: inputs(T, B, D) → 模型状态(B, M) → 批量计算 → 梯度累积 → 参数更新\n",
    "```\n",
    "\n",
    "其中：\n",
    "- `T`: 时间步长数\n",
    "- `B`: 批处理大小\n",
    "- `D`: 输入特征维度\n",
    "- `M`: 模型状态维度\n",
    "\n",
    "\n",
    "\n",
    "**🔑 批处理操作的三个核心要素**\n",
    "\n",
    "**1. 批量状态初始化 - 为什么这是关键？**\n",
    "\n",
    "```python\n",
    "brainstate.nn.init_all_states(self.model, batch_size=inputs.shape[1])\n",
    "```\n",
    "\n",
    "这一行代码实际上在做什么：\n",
    "\n",
    "```python\n",
    "# 原始状态（单样本）\n",
    "神经元电压: V → shape (200,)\n",
    "神经元脉冲: spike → shape (200,)\n",
    "\n",
    "# 批处理后（128个样本）\n",
    "神经元电压: V → shape (128, 200)\n",
    "神经元脉冲: spike → shape (128, 200)\n",
    "```\n",
    "\n",
    "为什么必须这样做？\n",
    "\n",
    "- RNN/SNN需要维护时间步之间的状态\n",
    "- 批处理意味着同时处理128个独立的序列\n",
    "- 每个序列都需要自己的状态副本\n",
    "\n",
    "**2. Batching模式 - 算法如何感知批处理？**\n",
    "\n",
    "```python\n",
    "mode=brainstate.mixin.Batching()\n",
    "```\n",
    "\n",
    "这个参数告诉ES_D_RTRL算法：\n",
    "\n",
    "```python\n",
    "# 没有Batching模式的期望\n",
    "输入: (100,) 单个样本\n",
    "输出: (10,) 单个预测\n",
    "\n",
    "# 有Batching模式的期望\n",
    "输入: (128, 100) 批量样本\n",
    "输出: (128, 10) 批量预测\n",
    "```\n",
    "\n",
    "**3. 时序批量处理 - 最复杂的部分**\n",
    "\n",
    "```python\n",
    "grads, (outs, losses) = brainstate.compile.scan(_etrace_step, grads, inputs)\n",
    "```\n",
    "\n",
    "这里发生的事情：\n",
    "\n",
    "```python\n",
    "# inputs shape: (50, 128, 100) - 50个时间步，每步128个样本\n",
    "\n",
    "时间步 0: 处理 inputs[0] → (128, 100) → 更新128个状态 → 计算梯度\n",
    "时间步 1: 处理 inputs[1] → (128, 100) → 更新128个状态 → 累积梯度\n",
    "...\n",
    "时间步 49: 处理 inputs[49] → (128, 100) → 更新128个状态 → 最终梯度\n",
    "```"
   ],
   "id": "1b89fc61f4e58162"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 自动批处理\n",
    "\n",
    "自动批处理主要使用`brainstate.transform.vmap`函数来实现。该函数可以将模型的更新函数向量化，从而实现批处理操作。\n",
    "\n",
    "### 核心特点\n",
    "\n",
    "自动批处理通过 `vmap` 函数实现向量化操作：\n",
    "\n",
    "1. **状态管理**：模型状态保持单样本形状 `(M)`\n",
    "2. **自动向量化**：`vmap` 自动处理批量维度映射\n",
    "3. **代码简洁**：减少手动批处理的复杂性\n",
    "\n",
    "### 具体示例"
   ],
   "id": "6ef11b809f8fa8af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:57:31.409938Z",
     "start_time": "2025-07-22T05:57:31.402666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainerAutoBatching:\n",
    "    \"\"\"自动批处理训练器\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        self.model = LIFNet(n_in, n_hidden, n_out)\n",
    "        self.optimizer = brainstate.optim.Adam(lr=1e-3)\n",
    "        self.optimizer.register_trainable_weights(self.model.states(brainstate.ParamState))\n",
    "\n",
    "    @brainstate.transform.jit(static_argnums=0)\n",
    "    def train(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        单轮训练步骤（自动批处理版本）\n",
    "\n",
    "        Args:\n",
    "            inputs: 输入序列 shape: (T, B, D)\n",
    "            targets: 目标标签 shape: (B,)\n",
    "        \"\"\"\n",
    "        # 步骤1：创建在线学习算法实例（无需手动批处理模式）\n",
    "        model = brainscale.ES_D_RTRL(self.model, decay_or_rank=0.9)\n",
    "\n",
    "        # 步骤2：使用vmap创建批量状态初始化函数\n",
    "        @brainstate.transform.vmap_new_states(\n",
    "            axis_size=inputs.shape[1],                   # 批处理大小\n",
    "            state_tag='new',                            # 状态标签（用于区分不同的状态组）\n",
    "        )\n",
    "        def init():\n",
    "            \"\"\"初始化单个样本的模型状态\"\"\"\n",
    "            brainstate.nn.init_all_states(self.model)\n",
    "            model.compile_graph(inputs[0, 0])           # 使用单个样本编译图\n",
    "\n",
    "        # 执行批量初始化\n",
    "        init()\n",
    "\n",
    "        # 步骤3：创建向量化模型包装器\n",
    "        vmap_model = brainstate.nn.Vmap(\n",
    "            model,\n",
    "            vmap_states='new'                           # 指定要向量化的状态组\n",
    "        )\n",
    "\n",
    "        # 步骤4：获取可训练参数\n",
    "        weights = self.model.states(brainstate.ParamState)\n",
    "\n",
    "        def _etrace_grad(inp):\n",
    "            \"\"\"计算单步的损失和梯度（自动向量化版本）\"\"\"\n",
    "            out = vmap_model(inp)                       # 自动处理批量维度\n",
    "            loss = braintools.metric.softmax_cross_entropy_with_integer_labels(\n",
    "                out, targets\n",
    "            ).mean()\n",
    "            return loss, out\n",
    "\n",
    "        def _etrace_step(prev_grads, x):\n",
    "            \"\"\"资格轨迹梯度累积步骤\"\"\"\n",
    "            f_grad = brainstate.augment.grad(\n",
    "                _etrace_grad, weights,\n",
    "                has_aux=True, return_value=True\n",
    "            )\n",
    "            cur_grads, local_loss, out = f_grad(x)\n",
    "            next_grads = jax.tree.map(lambda a, b: a + b, prev_grads, cur_grads)\n",
    "            return next_grads, (out, local_loss)\n",
    "\n",
    "        # 步骤5：时序前向传播与梯度累积\n",
    "        grads = jax.tree.map(u.math.zeros_like, weights.to_dict_values())\n",
    "        grads, (outs, losses) = brainstate.compile.scan(_etrace_step, grads, inputs)\n",
    "\n",
    "        # 步骤6：梯度裁剪与参数更新\n",
    "        grads = brainstate.functional.clip_grad_norm(grads, 1.0)\n",
    "        self.optimizer.update(grads)\n",
    "\n",
    "        return losses.mean()\n",
    "\n",
    "    def f_train(self, n_epochs, inputs, targets):\n",
    "        \"\"\"完整训练流程\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self.train(inputs, targets)\n",
    "            print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}')"
   ],
   "id": "e4bdc46b8921112d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:57:32.338706Z",
     "start_time": "2025-07-22T05:57:31.424973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建训练器并开始训练\n",
    "trainer_auto = TrainerAutoBatching(n_in, n_hidden, n_out)\n",
    "trainer_auto.f_train(10, xs, ys)"
   ],
   "id": "e3aa39eed4d8306b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.5856\n",
      "Epoch 2/10, Loss: 3.4918\n",
      "Epoch 3/10, Loss: 3.1237\n",
      "Epoch 4/10, Loss: 2.9487\n",
      "Epoch 5/10, Loss: 2.7401\n",
      "Epoch 6/10, Loss: 2.7286\n",
      "Epoch 7/10, Loss: 2.6906\n",
      "Epoch 8/10, Loss: 2.5279\n",
      "Epoch 9/10, Loss: 2.5536\n",
      "Epoch 10/10, Loss: 2.4651\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 代码详解\n",
    "\n",
    "\n",
    "**核心思想对比**\n",
    "\n",
    "在深入分析之前，让我们理解自动批处理与手动批处理的根本差异：\n",
    "\n",
    "| 维度 | 手动批处理 | 自动批处理 |\n",
    "|------|------------|------------|\n",
    "| **状态管理** | 显式批量状态 `(B, M)` | 单样本状态 `(M)` + 自动向量化 |\n",
    "| **计算方式** | 直接批量计算 | `vmap` 函数式映射 |\n",
    "| **代码复杂度** | 需要处理批量维度 | 抽象掉批量细节 |\n",
    "\n",
    "```python\n",
    "# 概念对比\n",
    "手动批处理思路: 创建128个神经元状态副本，同时计算\n",
    "自动批处理思路: 定义单个神经元计算逻辑，自动复制128次\n",
    "```\n",
    "\n",
    "\n",
    "**关键步骤详细解析**\n",
    "\n",
    "\n",
    "**步骤1：算法实例创建（简化模式）**\n",
    "\n",
    "```python\n",
    "# 步骤1：创建在线学习算法实例（无需手动批处理模式）\n",
    "model = brainscale.ES_D_RTRL(self.model, decay_or_rank=0.9)\n",
    "```\n",
    "\n",
    "**自动批处理关键点：**\n",
    "\n",
    "注意这里**没有**使用 `mode=brainstate.mixin.Batching()`：\n",
    "\n",
    "```python\n",
    "# 手动批处理版本\n",
    "model = brainscale.ES_D_RTRL(\n",
    "    self.model,\n",
    "    decay_or_rank=0.9,\n",
    "    mode=brainstate.mixin.Batching()  # 显式启用批处理\n",
    ")\n",
    "\n",
    "# 自动批处理版本\n",
    "model = brainscale.ES_D_RTRL(self.model, decay_or_rank=0.9)\n",
    "# 算法以为自己在处理单个样本！\n",
    "```\n",
    "\n",
    "**步骤2：vmap状态初始化（核心机制）- 状态向量化的魔法**\n",
    "\n",
    "```python\n",
    "# 步骤2：使用vmap创建批量状态初始化函数\n",
    "@brainstate.transform.vmap_new_states(\n",
    "    axis_size=inputs.shape[1],                   # 批处理大小\n",
    "    state_tag='new',                            # 状态标签（用于区分不同的状态组）\n",
    ")\n",
    "def init():\n",
    "    \"\"\"初始化单个样本的模型状态\"\"\"\n",
    "    brainstate.nn.init_all_states(self.model) # 只初始化1个！\n",
    "    model.compile_graph(inputs[0, 0])           # 使用单个样本编译图\n",
    "\n",
    "# 执行批量初始化\n",
    "init()\n",
    "```\n",
    "\n",
    "这里的关键理解：\n",
    "- 函数 `init()` 只知道如何初始化**1个样本**的状态\n",
    "- `vmap_new_states` 自动将这个函数**复制128次**\n",
    "- 结果：得到128个独立的状态副本，但代码只写了1个样本的逻辑\n",
    "- 状态标签 `'new'` 便于区分不同的状态组，确保后面便利地抽取出这些批量初始化的状态\n",
    "\n",
    "\n",
    "**步骤3：Vmap模型包装器 - 单样本→批量的自动转换**\n",
    "\n",
    "```python\n",
    "vmap_model = brainstate.nn.Vmap(model, vmap_states='new')\n",
    "\n",
    "# 调用时的内部流程：\n",
    "输入: (128, 100)\n",
    "  ↓ vmap自动分解\n",
    "128个并行计算: 每个处理 (100,) → (10,)\n",
    "  ↓ vmap自动组合\n",
    "输出: (128, 10)\n",
    "```\n",
    "\n",
    "当你调用 `vmap_model(inp)` 时：\n",
    "\n",
    "```python\n",
    "# inp.shape = (128, 100)\n",
    "\n",
    "# Step 1: vmap分解输入\n",
    "sample_0 = inp[0]   # (100,)\n",
    "sample_1 = inp[1]   # (100,)\n",
    "...\n",
    "sample_127 = inp[127] # (100,)\n",
    "\n",
    "# Step 2: 并行执行（概念上，实际可能向量化）\n",
    "result_0 = model_with_state_0(sample_0)   # (10,)\n",
    "result_1 = model_with_state_1(sample_1)   # (10,)\n",
    "...\n",
    "result_127 = model_with_state_127(sample_127) # (10,)\n",
    "\n",
    "# Step 3: vmap组合输出\n",
    "output = stack([result_0, result_1, ..., result_127])  # (128, 10)\n",
    "```\n",
    "\n",
    "**关键洞察：** 模型函数始终以为自己在处理单个样本，完全不知道批处理的存在！\n",
    "\n",
    "这种设计的美妙之处在于：你可以用最简单直观的方式编写神经网络逻辑（单样本），然后自动获得高效的批处理能力。\n"
   ],
   "id": "31ab5d7c87ceaae3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 总结\n",
    "\n",
    "BrainScale 的两种批处理策略各有优势。手动批处理提供了更精细的控制和更高的性能，适合生产环境的大规模训练；自动批处理则以简洁的API降低了实现复杂度，更适合研究和原型开发。\n",
    "\n",
    "选择合适的批处理策略需要综合考虑具体的应用场景、性能要求和开发效率。建议在项目初期使用自动批处理快速验证想法，在性能优化阶段考虑迁移到手动批处理。"
   ],
   "id": "c58a3f4e5126d0f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
